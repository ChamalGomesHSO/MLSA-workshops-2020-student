{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop SL01: Classification\n",
    "\n",
    "## Agenda\n",
    "- Introduction to training and testing data distribution\n",
    "- Common classification models\n",
    "\n",
    "## Previously on the last 2 workshops\n",
    "From the last 2 workshops we have covered the pre-processing of data before model training: \n",
    "- Read data into dataframes\n",
    "- Join multiple dataframes\n",
    "- Encode string data into float/int\n",
    "- Feature selection/engineering \n",
    "\n",
    "## Exercise\n",
    "- Think about how to tune hyperparameters for better performance (hint: Sklearn Documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepping the data\n",
    "These are from last 2 workshops straight, to get the dataframe to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv file into a dataframe\n",
    "df_id_train = pd.read_csv(\"train_identity.csv\")\n",
    "df_tran_train = pd.read_csv(\"train_transaction.csv\")\n",
    "df_id_test = pd.read_csv(\"test_identity.csv\")\n",
    "df_tran_test = pd.read_csv(\"test_transaction.csv\")\n",
    "\n",
    "# joining table\n",
    "df_train = pd.merge(df_tran_train,df_id_train, on='TransactionID' ,how='left')\n",
    "\n",
    "# target dataframe \n",
    "Y_train = df_train['isFraud']\n",
    "Y_train = pd.DataFrame(Y_train)\n",
    "\n",
    "# dropping the irrelevant data for training\n",
    "list = ['isFraud','TransactionID','DeviceInfo']\n",
    "X_train = df_train.drop(list, axis=1)\n",
    "\n",
    "# encoding strings\n",
    "obj_df = X_train.select_dtypes(include=['object']).copy()\n",
    "int_df = X_train.select_dtypes(include=['int64']).copy()\n",
    "float_df = X_train.select_dtypes(include=['float64']).copy()\n",
    "\n",
    "for column in obj_df.head(0):\n",
    "    obj_df[column] = obj_df[column].astype('category')\n",
    "    obj_df[column] = obj_df[column].cat.codes\n",
    "\n",
    "X_train = pd.concat([obj_df,int_df,float_df],axis=1, sort=False) \n",
    "\n",
    "# filling na\n",
    "X_train.fillna(value=-1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can just download the dataframe as csv for future use. We only need to download it once, so in the future we just need to read these csv into dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloadig dataframe as csv\n",
    "X_train.to_csv (r'X_train.csv', index = None, header=True)\n",
    "Y_train.to_csv (r'Y_train.csv', index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('X_train.csv')\n",
    "Y_train = pd.read_csv('Y_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing/Training Set Distribution\n",
    "\n",
    "For **model selection** purpose we need to distribute the data into training and testing set, and compute model error on both sets, i.e. train error and test error. If we select model based on train error solely, we will have over-fitting problem because the model will just perform really well on training data but not on testing data. Test error is a better tool to judge whether the model will perform well on new data. Perhaps this graph will explain better.\n",
    "\n",
    "<img src=\"train-test-error.png\">\n",
    "\n",
    "Source: [In-depth introduction to machine learning in 15 hours of expert videos](https://www.r-bloggers.com/in-depth-introduction-to-machine-learning-in-15-hours-of-expert-videos/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting test/train data into 80:20\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_size = int(0.8*X_train.shape[0])\n",
    "test_size = X_train.shape[0]-train_size\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_train, Y_train, train_size=train_size, test_size=test_size, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is optional: run this if you don't want to read the future warning messages\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "# this is to stop the format warning\n",
    "Y_train = np.array(Y_train).ravel()\n",
    "Y_test = np.array(Y_test).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "There are many models up our sleeves. We provide a list of models here for you to explore here:\n",
    "- [Generalized Linear Models](https://scikit-learn.org/stable/modules/linear_model.html) (Logistic regression, [SGD](https://scikit-learn.org/stable/modules/sgd.html), Perceptron) \n",
    "- [Linear and Quadratic Discriminant Analysis](https://scikit-learn.org/stable/modules/lda_qda.html#dimensionality-reduction-using-linear-discriminant-analysis)\n",
    "- [Support Vector Machines](https://scikit-learn.org/stable/modules/svm.html) (SVC)\n",
    "- [Nearest Neighbors](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification)\n",
    "- [Gaussian Processes](https://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process-classification-gpc)\n",
    "- [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "- [Trees](https://scikit-learn.org/stable/modules/tree.html)\n",
    "- [Ensemble Methods](https://scikit-learn.org/stable/modules/ensemble.html) \n",
    "- Neural Network (more about this in later workshops) \n",
    "\n",
    "If anyone in interested in the math, you can click on the links and read more.\n",
    "\n",
    "Later when we talk about regression we will notice the list for regression problems is pretty similar to this one. Actually a classification problem can be viewed as a regression problem, with the regression output being the probability of being classified into a specific category. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) GLM\n",
    "**Logistic regression** is a linear model for classification, with the probability of being a specific category being modeled using a logistic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.964863  0.999798  0.982020    113954\n",
      "           1   0.178571  0.001204  0.002391      4154\n",
      "\n",
      "   micro avg   0.964676  0.964676  0.964676    118108\n",
      "   macro avg   0.571717  0.500501  0.492206    118108\n",
      "weighted avg   0.937208  0.964676  0.947565    118108\n",
      "\n",
      "[[113931     23]\n",
      " [  4149      5]]\n",
      "0.9646763978731331\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# training lr model\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, Y_train)\n",
    "# predict on test data\n",
    "Y_lr = lr.predict(X_test)\n",
    "# accuracy \n",
    "print (classification_report(Y_test, Y_lr,digits = 6))\n",
    "print (confusion_matrix(Y_test, Y_lr))\n",
    "print (accuracy_score(Y_test, Y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this is very bad because the model is very bad at detecting fraud transactions (only a few out of over 4000 fraud transactions). Even with cross validation it is still pretty bad. (When we say this you can believe us because we tried and there are only slightly more successful fraud detections). \n",
    "\n",
    "Perhaps there is a way to fix this by putting more penalty on false negative? This is an exercie for you to find out how! ([hint](https://stackoverflow.com/questions/49151325/how-to-penalize-false-negatives-more-than-false-positives))\n",
    "\n",
    "If you have forgotten how to read confusion matrix, here is a [link](https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62) on how. <img src=\"cm.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **perceptron** is another simple classification algorithm that works well for large scale learning. The **passive-aggressive algorithms** are a family of algorithms for large-scale learning. They are similar to the Perceptron but do not require a learning rate and include a regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.964931  0.997955  0.981165    113954\n",
      "           1   0.082677  0.005055  0.009528      4154\n",
      "\n",
      "   micro avg   0.963034  0.963034  0.963034    118108\n",
      "   macro avg   0.523804  0.501505  0.495347    118108\n",
      "weighted avg   0.933901  0.963034  0.946992    118108\n",
      "\n",
      "[[113721    233]\n",
      " [  4133     21]]\n",
      "0.9630338334405798\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "# training model\n",
    "pct = Perceptron()\n",
    "pct.fit(X_train, Y_train)\n",
    "# predict on test data\n",
    "Y_pct = pct.predict(X_test)\n",
    "# accuracy \n",
    "print (classification_report(Y_test, Y_pct,digits = 6))\n",
    "print (confusion_matrix(Y_test, Y_pct))\n",
    "print (accuracy_score(Y_test, Y_pct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.965120  0.993603  0.979154    113954\n",
      "           1   0.078382  0.014925  0.025076      4154\n",
      "\n",
      "   micro avg   0.959181  0.959181  0.959181    118108\n",
      "   macro avg   0.521751  0.504264  0.502115    118108\n",
      "weighted avg   0.933932  0.959181  0.945598    118108\n",
      "\n",
      "[[113225    729]\n",
      " [  4092     62]]\n",
      "0.9591814271683544\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "# training model\n",
    "pac = PassiveAggressiveClassifier()\n",
    "pac.fit(X_train, Y_train)\n",
    "# predict on test data\n",
    "Y_pac = pac.predict(X_test)\n",
    "# accuracy \n",
    "print (classification_report(Y_test, Y_pac,digits = 6))\n",
    "print (confusion_matrix(Y_test, Y_pac))\n",
    "print (accuracy_score(Y_test, Y_pac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SGD** or stochastic gradient descent is a simple yet very efficient approach to fit linear models (e.g. linear SVM, Logistic Regression etc.) by updating the model along with a decreasing strength schedule (aka learning rate). So SGD itself is not a model but an algorithm that minimises the loss function. In later workshop we will revisit SGD for fitting neural networks. Another great thing is that SGD allows minibatch (online/out-of-core) learning, see the `partial_fit` method. For best results using the default learning rate schedule, the data should be standardised (zero mean and unit variance). Hence, it is particularly useful when the number of samples (and the number of features) is very large. Learn more [here](https://scikit-learn.org/stable/modules/sgd.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.964935  0.997832  0.981108    113954\n",
      "           1   0.081784  0.005296  0.009948      4154\n",
      "\n",
      "   micro avg   0.962924  0.962924  0.962924    118108\n",
      "   macro avg   0.523360  0.501564  0.495528    118108\n",
      "weighted avg   0.933874  0.962924  0.946951    118108\n",
      "\n",
      "[[113707    247]\n",
      " [  4132     22]]\n",
      "0.9629237646899448\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "# training SGD model\n",
    "SGD = SGDClassifier()\n",
    "SGD.fit(X_train, Y_train)\n",
    "# predict on test data\n",
    "Y_SGD = SGD.predict(X_test)\n",
    "# accuracy \n",
    "print (classification_report(Y_test, Y_SGD,digits = 6))\n",
    "print (confusion_matrix(Y_test, Y_SGD))\n",
    "print (accuracy_score(Y_test, Y_SGD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though it's trained really fast, can you see the problem here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Linear and Quadratic Discriminant Analysis\n",
    "\n",
    "Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) are two classic classifiers, with, as their names suggest, a linear and a quadratic decision surface, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.976051  0.989970  0.982961    113954\n",
      "           1   0.548043  0.333654  0.414784      4154\n",
      "\n",
      "   micro avg   0.966886  0.966886  0.966886    118108\n",
      "   macro avg   0.762047  0.661812  0.698872    118108\n",
      "weighted avg   0.960997  0.966886  0.962978    118108\n",
      "\n",
      "[[112811   1143]\n",
      " [  2768   1386]]\n",
      "0.9668862397128052\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# training model\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, Y_train)\n",
    "# predict on test data\n",
    "Y_lda = lda.predict(X_test)\n",
    "# accuracy \n",
    "print (classification_report(Y_test, Y_lda,digits = 6))\n",
    "print (confusion_matrix(Y_test, Y_lda))\n",
    "print (accuracy_score(Y_test, Y_lda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:692: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.982177  0.911587  0.945567    113954\n",
      "           1   0.183814  0.546221  0.275064      4154\n",
      "\n",
      "   micro avg   0.898737  0.898737  0.898737    118108\n",
      "   macro avg   0.582996  0.728904  0.610315    118108\n",
      "weighted avg   0.954098  0.898737  0.921984    118108\n",
      "\n",
      "[[103879  10075]\n",
      " [  1885   2269]]\n",
      "0.898736749415789\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "# training model\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda.fit(X_train, Y_train)\n",
    "# predict on test data\n",
    "Y_qda = qda.predict(X_test)\n",
    "# accuracy \n",
    "print (classification_report(Y_test, Y_qda,digits = 6))\n",
    "print (confusion_matrix(Y_test, Y_qda))\n",
    "print (accuracy_score(Y_test, Y_qda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.964888  0.998613  0.981461    113954\n",
      "           1   0.076023  0.003130  0.006012      4154\n",
      "\n",
      "   micro avg   0.963601  0.963601  0.963601    118108\n",
      "   macro avg   0.520456  0.500871  0.493736    118108\n",
      "weighted avg   0.933626  0.963601  0.947153    118108\n",
      "\n",
      "[[113796    158]\n",
      " [  4141     13]]\n",
      "0.9636011108476987\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "# training model\n",
    "# svc = svm.SVC(kernel='linear')\n",
    "# SVC using the libsvm is gonna take forever to run because the core of an SVM is a quadratic programming problem (QP) \n",
    "# (about hours, we haven't run it ourselves but you can try)\n",
    "# SGDclassifier has the same cost function as linear SVC by adjusting penalty and loss parameters\n",
    "# in fact, the default SGDClasifier is a linear SVM\n",
    "svc = SGDClassifier(loss='hinge', penalty='l2')\n",
    "svc.fit(X_train, Y_train)\n",
    "# predict on test data\n",
    "Y_svc = svc.predict(X_test)\n",
    "# accuracy \n",
    "print (classification_report(Y_test, Y_svc,digits = 6))\n",
    "print (confusion_matrix(Y_test, Y_svc))\n",
    "print (accuracy_score(Y_test, Y_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.966557  0.996736  0.981414    113954\n",
      "           1   0.375839  0.053924  0.094316      4154\n",
      "\n",
      "   micro avg   0.963576  0.963576  0.963576    118108\n",
      "   macro avg   0.671198  0.525330  0.537865    118108\n",
      "weighted avg   0.945780  0.963576  0.950214    118108\n",
      "\n",
      "[[113582    372]\n",
      " [  3930    224]]\n",
      "0.9635757103667829\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "# training knn model\n",
    "knn = neighbors.KNeighborsClassifier()\n",
    "knn.fit(X_train, Y_train)\n",
    "# Predict on test data\n",
    "Y_knn = knn.predict(X_test)\n",
    "# accuracy \n",
    "print (classification_report(Y_test, Y_knn,digits = 6))\n",
    "print (confusion_matrix(Y_test, Y_knn))\n",
    "print (accuracy_score(Y_test, Y_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Gaussian Processes\n",
    "A generic supervised learning method designed to solve regression and probabilistic classification problems but loses efficiency in high dimensional spaces (when the number of features exceeds a few dozens). So this is not particularly useful for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.959500  1.000000  0.979331      1919\n",
      "           1   0.000000  0.000000  0.000000        81\n",
      "\n",
      "   micro avg   0.959500  0.959500  0.959500      2000\n",
      "   macro avg   0.479750  0.500000  0.489666      2000\n",
      "weighted avg   0.920640  0.959500  0.939669      2000\n",
      "\n",
      "[[1919    0]\n",
      " [  81    0]]\n",
      "0.9595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "# training model\n",
    "mini_size = 2000\n",
    "gpc = GaussianProcessClassifier(1.0 * RBF(1.0),n_jobs=-1)\n",
    "gpc.fit(X_train.iloc[0:mini_size], Y_train[0:mini_size])\n",
    "# Predict on test data\n",
    "Y_gcp = gpc.predict(X_test.iloc[0:mini_size])\n",
    "# accuracy \n",
    "print (classification_report(Y_test[0:mini_size], Y_gcp,digits = 6))\n",
    "print (confusion_matrix(Y_test[0:mini_size], Y_gcp))\n",
    "print (accuracy_score(Y_test[0:mini_size], Y_gcp))\n",
    "# we have reduced the dataset to mini size otherwise the kernel will die\n",
    "# the warning message appears because the model never predicts a fraud setection  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) Naive Bayes\n",
    "[Scikit learn document](https://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes): \"Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes theorem with the “naive” assumption of conditional independence between every pair of features given the value of the class variable. In spite of their apparently over-simplified assumptions, naive Bayes classifiers have worked quite well in many real-world situations, famously document classification and spam filtering.\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.952437  0.028643  0.055614    113954\n",
      "           1   0.034801  0.960761  0.067169      4154\n",
      "\n",
      "   micro avg   0.061427  0.061427  0.061427    118108\n",
      "   macro avg   0.493619  0.494702  0.061391    118108\n",
      "weighted avg   0.920162  0.061427  0.056020    118108\n",
      "\n",
      "[[  3264 110690]\n",
      " [   163   3991]]\n",
      "0.06142682968130863\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "# training model\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, Y_train)\n",
    "# Predict on test data\n",
    "Y_gnb = gnb.predict(X_test)\n",
    "# accuracy \n",
    "print (classification_report(Y_test, Y_gnb,digits = 6))\n",
    "print (confusion_matrix(Y_test, Y_gnb))\n",
    "print (accuracy_score(Y_test, Y_gnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7) Trees\n",
    "Tree models do not require any parameters but predict the target by applying a set of if-then-else decision rules to the features. The deeper the tree, the more complex the decision rules,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.984969  0.986231  0.985600    113954\n",
      "           1   0.608533  0.587145  0.597648      4154\n",
      "\n",
      "   micro avg   0.972195  0.972195  0.972195    118108\n",
      "   macro avg   0.796751  0.786688  0.791624    118108\n",
      "weighted avg   0.971730  0.972195  0.971955    118108\n",
      "\n",
      "[[112385   1569]\n",
      " [  1715   2439]]\n",
      "0.9721949402242016\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "# training model\n",
    "dt = tree.DecisionTreeClassifier(max_depth=50)\n",
    "dt.fit(X_train, Y_train)\n",
    "# Predict on test data\n",
    "Y_dt = dt.predict(X_test)\n",
    "# accuracy \n",
    "print (classification_report(Y_test, Y_dt,digits = 6))\n",
    "print (confusion_matrix(Y_test, Y_dt))\n",
    "print (accuracy_score(Y_test, Y_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8) Ensemble Methods\n",
    "Lastly, ensemble method is where we combine the predictions of estimators built with a given learning algorithm together. The goal of ensemble methods is to improve generalizability and robustness over a single estimator.\n",
    "\n",
    "There are two families of ensemble methods:\n",
    "\n",
    "- **Averaging methods**: build several estimators independently and then average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.\n",
    "\n",
    "- Examples: Bagging methods, Forests of randomized trees (Random Forests, Extremely Randomized Trees, Totally Random Trees Embedding), …\n",
    "\n",
    "- **Boosting methods**: base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.\n",
    "\n",
    "- Examples: AdaBoost, Gradient Tree Boosting, Voting Classifier …\n",
    "\n",
    "Bagging methods work best with strong and complex models (e.g. fully developed decision trees), whereas boosting methods usually work best with weak models (e.g. shallow decision trees)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.964972  0.996490  0.980477    113954\n",
      "           1   0.074074  0.007703  0.013956      4154\n",
      "\n",
      "   micro avg   0.961713  0.961713  0.961713    118108\n",
      "   macro avg   0.519523  0.502097  0.497217    118108\n",
      "weighted avg   0.933638  0.961713  0.946484    118108\n",
      "\n",
      "[[113554    400]\n",
      " [  4122     32]]\n",
      "0.9617130084329597\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "# training model\n",
    "bag = BaggingClassifier(SGDClassifier())\n",
    "bag.fit(X_train, Y_train)\n",
    "# Predict on test data\n",
    "Y_bag = bag.predict(X_test)\n",
    "# accuracy \n",
    "print (classification_report(Y_test, Y_bag,digits = 6))\n",
    "print (confusion_matrix(Y_test, Y_bag))\n",
    "print (accuracy_score(Y_test, Y_bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.979656  0.998956  0.989212    113954\n",
      "           1   0.937664  0.430910  0.590467      4154\n",
      "\n",
      "   micro avg   0.978977  0.978977  0.978977    118108\n",
      "   macro avg   0.958660  0.714933  0.789839    118108\n",
      "weighted avg   0.978179  0.978977  0.975187    118108\n",
      "\n",
      "[[113835    119]\n",
      " [  2364   1790]]\n",
      "0.9789768686287127\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# training model\n",
    "rf = RandomForestClassifier(max_depth=50,n_estimators=20)\n",
    "rf.fit(X_train, Y_train)\n",
    "# Predict on test data\n",
    "Y_rf = rf.predict(X_test)\n",
    "# accuracy \n",
    "print (classification_report(Y_test, Y_rf,digits = 6))\n",
    "print (confusion_matrix(Y_test, Y_rf))\n",
    "print (accuracy_score(Y_test, Y_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.973777  0.997499  0.985495    113954\n",
      "           1   0.793179  0.263120  0.395155      4154\n",
      "\n",
      "   micro avg   0.971670  0.971670  0.971670    118108\n",
      "   macro avg   0.883478  0.630309  0.690325    118108\n",
      "weighted avg   0.967425  0.971670  0.964732    118108\n",
      "\n",
      "[[113669    285]\n",
      " [  3061   1093]]\n",
      "0.9716699969519423\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# training model\n",
    "ab = AdaBoostClassifier(n_estimators=20) # default is DecisionTreeClassifier(max_depth=1)\n",
    "ab.fit(X_train, Y_train)\n",
    "# Predict on test data\n",
    "Y_ab = ab.predict(X_test)\n",
    "# accuracy \n",
    "print (classification_report(Y_test, Y_ab,digits = 6))\n",
    "print (confusion_matrix(Y_test, Y_ab))\n",
    "print (accuracy_score(Y_test, Y_ab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.972438  0.998833  0.985459    113954\n",
      "           1   0.874647  0.223399  0.355896      4154\n",
      "\n",
      "   micro avg   0.971560  0.971560  0.971560    118108\n",
      "   macro avg   0.923542  0.611116  0.670678    118108\n",
      "weighted avg   0.968999  0.971560  0.963316    118108\n",
      "\n",
      "[[113821    133]\n",
      " [  3226    928]]\n",
      "0.9715599282013073\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# training model\n",
    "gb = GradientBoostingClassifier(n_estimators=20)\n",
    "gb.fit(X_train, Y_train)\n",
    "# Predict on test data\n",
    "Y_gb = gb.predict(X_test)\n",
    "# accuracy \n",
    "print (classification_report(Y_test, Y_gb,digits = 6))\n",
    "print (confusion_matrix(Y_test, Y_gb))\n",
    "print (accuracy_score(Y_test, Y_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hope you enjoyed this session so far. As you can see, different models have very different performance, and perhaps it's a good idea to take some time to research on what hyperparameters to tune and how to better manipulate data (feature selection, feature engineering, stacking etc.) to feed the model. [Here](https://www.kaggle.com/c/ieee-fraud-detection/discussion/111284#latest-655464) is the first place solution for IEEE fraud detection competition. Maybe this will give you some ideas on how to better manipulation data and train models.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlsa",
   "language": "python",
   "name": "mlsa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
