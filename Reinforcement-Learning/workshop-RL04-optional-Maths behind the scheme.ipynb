{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Workshop RL04: Maths behind the scheme \n",
    "\n",
    "This oprional workshop digs deeper into the details of RL mathematics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Based\n",
    "In a model-based RL, we want to give the agent a model of how the world works (or how it should perceive how the world works). That is, we want to construct mathematical models for state dynamics and rewards. Often we model the states dynamics as a **Markov Process** and model the rewards as the expectations of rewards.  \n",
    "\n",
    "#### 1. Markov Process\n",
    "Tp construct a Markov process for states dynamics, we assume that the state is Markov. That is, we assume that the state provides just enough information to represent history (i.e. $s_t$ is sufficient statistic of history $h_t$).\n",
    "\n",
    "So from that we can get:\n",
    "$$p(s_{t+1}|{h_t,a_t}) = p(s_{t+1}|{s_t,a_t})$$\n",
    "recall that $h_t$ contains all information from time 0 to time $t$, whereas $s_t$ contains only the current information. Here we replaced full information with current information to determine future event, (i.e. future is independent of past present). Here we have statisfied the definition of Markov. \n",
    "\n",
    "Why use Markov assumption:\n",
    "- so agent makes decision based only on current information\n",
    "    - computational complexity\n",
    "    - data required\n",
    "- can always be satisfied by equating $s_t$ and $h_t$, i.e. $s_t = h_t$\n",
    "- resulting performance\n",
    "\n",
    "Now how do state dymanics look like:\n",
    "\n",
    "\n",
    "#### 2. Markov Reward Process\n",
    "Now plug in the the model for rewards, i.e. $R=E \\lbrack r_t|s_t \\rbrack$, we can construct a **Markov Reward Process (MRP)**:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 3. Markov Decision Process\n",
    "Lastly, let's plug in actions, and we have **Markov Decision Process (MDP)**:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Depending on how agent state $s_t$ is defined, we have different types of MDP:\n",
    "- Full Observability / Markov Decision Process (MDP):\n",
    "    - Agent state uses observation from world state: $s_t = o_t$\n",
    "    - that is, we assume current observation is sufficient statistic of history, i.e. $o_t = h_t$ \n",
    "- Partial Observability / Partially Observable Markov Decision Process (POMDP):\n",
    "    - Agent state is not the same as the world state\n",
    "    - Agent contructs its own state: use $s_t = h_t$, or RNN, or beliefs of world state ...\n",
    "    - when?: when we don't have access to all information like in poker game\n",
    "- Bandits:\n",
    "    - actions have no influence on next observations (so actions don't affect real world) \n",
    "    - so actions are independent of one another \n",
    "    - there are no delayed rewards\n",
    "    - as opposed to bandits, MDP and POMDP actions influence next observation (so credict assignment and strategic actions might required)\n",
    "- Continuous-state MDP:\n",
    "    - requires optimal control\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlsa",
   "language": "python",
   "name": "mlsa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
