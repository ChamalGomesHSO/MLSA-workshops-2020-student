{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Workshop RL04: Maths behind the scheme \n",
    "\n",
    "This oprional workshop digs deeper into the details of RL mathematics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Based\n",
    "In a model-based RL, we want to give the agent a model of how the world works (or how it should perceive how the world works). That is, we want to construct mathematical models for state dynamics and rewards. Often we model the states dynamics as a **Markov Process** and model the rewards as the expectations of future rewards.  \n",
    "\n",
    "### 1. Markov Process\n",
    "#### Assumption\n",
    "------------\n",
    "To construct a Markov process for states dynamics, we assume that the state is Markov. That is, we assume that the state provides just enough information to represent history (i.e. $s_t$ is sufficient statistic of history $h_t$).\n",
    "\n",
    "So from that we can get:\n",
    "$$p(s_{t+1}|{h_t,a_t}) = p(s_{t+1}|{s_t,a_t})$$\n",
    "Recall that $h_t$ contains all information from time 0 to time $t$, whereas $s_t$ contains only the current information. Here we replaced full information with current information to determine future event, (i.e. future is independent of past present). \n",
    "\n",
    "Hence we have statisfied the definition of Markov, and thus we can say the states dynamic is a Markov process or Markov chain.\n",
    "\n",
    ">Why use Markov assumption:\n",
    "- so agent makes decision based only on current information\n",
    "    - computational complexity\n",
    "    - data required\n",
    "- can always be satisfied by equating $s_t$ and $h_t$, i.e. $s_t = h_t$\n",
    "- resulting performance\n",
    "\n",
    "#### Definition \n",
    "-----------\n",
    "Now let's define things more formally, a Markov Process is:\n",
    "\n",
    "| Markov Process|\n",
    "|:--------------|\n",
    "|$S$ is a set of states $s_t \\in S$|\n",
    "|$P$ is dynamics/transition model that specifies $p(s_{t+1} = s'|{s_t=s})$.|\n",
    "\n",
    "Suppose we have $N$ states $s_1, s_2, ..., s_N$, state dymanics look like: \n",
    "        $$ P = \n",
    "        \\begin{bmatrix}\n",
    "            p(s_1|s_1) & p(s_2|s_1) & p(s_3|s_1) & \\dots  & p(s_N|s_1) \\\\\n",
    "            p(s_1|s_2) & p(s_2|s_2) & p(s_3|s_2) & \\dots  & p(s_N|s_2) \\\\\n",
    "            \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "            p(s_1|s_N) & p(s_2|s_N) & p(s_3|s_N) & \\dots  & p(s_N|s_N)\n",
    "        \\end{bmatrix}$$\n",
    "\n",
    "> Note that the uderscript here doesn't mean time step anymore, but represents different states in the world. So the first row are the probabilities of tranfering to other states from state 1 $s_1$. \n",
    "\n",
    "If we simulate the state transitions, we can sample **episodes** like these:\n",
    "- $s_1,s_2,s_3,s_2,s_3,s_4,s_5.s_5,s_5,...$\n",
    "- $s_1,s_2,s_3,s_3,s_4,s_5,s_6.s_6,s_7,...$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Markov Reward Process (MRP)\n",
    "#### Defintion\n",
    "-----------\n",
    "Now plug in the rewards to define a **Markov Reward Process**, i.e.:\n",
    "\n",
    "\n",
    "| MRP |\n",
    "|:    |\n",
    "|$S$ is a set of states $s_t \\in S$|\n",
    "|$P$ is dynamics/transition model that specifies $p(s'|{s})$|\n",
    "|$R$ is a reward function that specifies current reward $R(s_t = s)=E \\lbrack r_t|s_t = s \\rbrack$.| \n",
    "|$\\gamma$ is the discount factor that $\\in [0,1]$|\n",
    "\n",
    "What $R$ looks like depends on the environment. In the frozen lake environment that we use in the exercise, it rewards the agent with 1 if it gets to the safe state, and 0 reward for other states, i.e.: $$R(s_1) = R(s_2) = ... R(s_{N-1}) = 0$$ $$R(s_N) = 1$$\n",
    "    \n",
    ">Note again that the uderscript here doesn't mean time step, but represents different states in the world. So the current reward of being in state 1 $s_1$ is 0. \n",
    "\n",
    "#### Value Function \n",
    "-----------\n",
    "Now with rewards we can compute the value function: $$V(s_t = s) = E \\lbrack r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + ... |s_t = s \\rbrack,$$ which has 3 ways of computing:\n",
    "- by **simulation**: \n",
    "    - simulate a large amount of episodes \n",
    "    - compute the value function for each episode\n",
    "    - take the average of those \n",
    "    - pros: requires no states dynamics\n",
    "- by **analytical way**:\n",
    "    - using Markov property: \n",
    "        $$V(s) = E \\lbrack r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + ... |s_t = s \\rbrack$$\n",
    "\n",
    "        $$ = E \\lbrack r_t|s_t = s \\rbrack + \\gamma E \\lbrack r_{t+1} + \\gamma r_{t+2} + ... |s_t = s \\rbrack $$ \n",
    "\n",
    "        $$ = R(s) + \\gamma \\sum_{s' \\in S} p(s'|s)E \\lbrack r_{t} + \\gamma r_{t+1} + ... |s_{t} = s' \\rbrack$$\n",
    "\n",
    "        $$ = R(s) + \\gamma \\sum_{s' \\in S} p(s'|s)V(s')$$\n",
    "    - expressing it in matrixes:  \n",
    "        $$\\begin{bmatrix}\n",
    "            V(s_1)  \\\\\n",
    "            V(s_2)  \\\\\n",
    "            \\vdots \\\\\n",
    "            V(s_N) \n",
    "        \\end{bmatrix} = \n",
    "        \\begin{bmatrix}\n",
    "            R(s_1)  \\\\\n",
    "            R(s_2)  \\\\\n",
    "            \\vdots \\\\\n",
    "            R(s_N) \n",
    "        \\end{bmatrix} +\n",
    "        \\begin{bmatrix}\n",
    "            p(s_1|s_1) & p(s_2|s_1) & p(s_3|s_1) & \\dots  & p(s_N|s_1) \\\\\n",
    "            p(s_1|s_2) & p(s_2|s_2) & p(s_3|s_2) & \\dots  & p(s_N|s_2) \\\\\n",
    "            \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "            p(s_1|s_N) & p(s_2|s_N) & p(s_3|s_N) & \\dots  & p(s_N|s_N)\n",
    "        \\end{bmatrix}\n",
    "        \\begin{bmatrix}\n",
    "            V(s_1)  \\\\\n",
    "            V(s_2)  \\\\\n",
    "            \\vdots \\\\\n",
    "            V(s_N) \n",
    "        \\end{bmatrix}$$\n",
    "\n",
    "        $$ \\Rightarrow V = R + \\gamma PV $$\n",
    "\n",
    "        $$ \\Rightarrow V - \\gamma PV = R $$\n",
    "\n",
    "        $$ \\Rightarrow (1 - \\gamma P)V = R $$\n",
    "\n",
    "        $$ \\Rightarrow V = (I - \\gamma P)^{-1}R $$\n",
    "\n",
    "    - pros and cons: solving $V$ directly but requires $O(N^3)$    \n",
    "\n",
    "- by **iterative(dynamic programming) way**:\n",
    "    - for all $s \\in S$\n",
    "    - initiate $V_0(s) = 0$ \n",
    "    - start with k = 1 until converge: $$ V_k(s) = R(s) + \\gamma \\sum_{s' \\in S} p(s'|s)V_{k-1}(s') $$ \n",
    "    \n",
    "        - e.g. $V_k(s_1) = R(s_1) + \\gamma \\begin{bmatrix}\n",
    "            p(s_1|s_1) & p(s_2|s_1) & p(s_3|s_1) & \\dots  & p(s_N|s_1)\n",
    "        \\end{bmatrix} \\begin{bmatrix}\n",
    "            V_{k-1}(s_1)  \\\\\n",
    "            V_{k-1}(s_2)  \\\\\n",
    "            \\vdots \\\\\n",
    "            V_{k-1}(s_N) \n",
    "        \\end{bmatrix}$\n",
    "    - pros and cons: $O(N^2)$ for each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Markov Decision Process\n",
    "**Markov Decision Process** is **Markov Reward Process plus actions**.\n",
    "\n",
    "#### Definition\n",
    "--------\n",
    "|MDP|\n",
    "|:--|\n",
    "|$S$ is a set of states $s_t \\in S$|\n",
    "|$A$ is a set of actions $a \\in A $|\n",
    "|$P$ is dynamics/transition model that specifies $p(s'|{s,a})$ for **each state and each action** |\n",
    "|$R$ is a reward function that specifies current reward $R(s,a)$ for **each state and each action** |\n",
    "|$\\gamma$ is the discount factor that $\\in [0,1]$|\n",
    "\n",
    "For each state, $P$ and $R$ are defined differently for each action, e.g. for $s_1,a_1$, \n",
    "$$ P(s_1,a_1) = \\begin{bmatrix} p(s_1|s_1,a_1) & p(s_2|s_1,a_1) & \\dots & p(s_N|s_1,a_1) \\end{bmatrix} \\\\ \n",
    "            R(s_1,a_1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Markov Desicion Process + policy\n",
    "We often navigate the actions in **MDP with policy**, otherwise the agent would just take random actions. Policy can be deterministic or stochastic, but in general it's expressed as a conditional distribution, i.e. $\\pi(a|s) = p(a|s)$.\n",
    "#### Policy\n",
    "---------\n",
    "- **deterministic policy**: \n",
    "\n",
    "    For example, for a policy that says \"from state 1, must take action 1\": $$\\pi(a_1|s_1) = 1. $$    \n",
    "    If taking action 1 ends up in state 2, then the states dynamics for $s_1$ is    \n",
    "    $$ P(s_1,a_1) = \\begin{bmatrix}\n",
    "                p(s_1|s_1,a_1) & p(s_2|s_1,a_1) & \\dots & p(s_N|s_1,a_1)\n",
    "            \\end{bmatrix} = \n",
    "            \\begin{bmatrix}\n",
    "                0 & 1 & \\dots & 0\n",
    "            \\end{bmatrix} $$ \n",
    "     $$ P(s_1,a_2) = \\begin{bmatrix}\n",
    "                p(s_1|s_1,a_2) & p(s_2|s_1,a_2) & \\dots & p(s_N|s_1,a_2)\n",
    "            \\end{bmatrix} = \n",
    "            \\begin{bmatrix}\n",
    "                0 & 0 & \\dots & 0\n",
    "            \\end{bmatrix} $$ $$ \\dots $$\n",
    "            \n",
    "- **stochastic policy**: \n",
    "\n",
    "    For example, for a policy that says \"from state 1, take action 1 or action 2 with same probabilities\": $$\\pi(a_1|s_1) = \\pi(a_2|s_1) = 0.5. $$    \n",
    "    If taking action 1 ends up in state 2, and action 2 stays in state 1, then the states dynamics for $s_1$ is    \n",
    "    $$ P(s_1,a_1) = \\begin{bmatrix}\n",
    "                p(s_1|s_1,a_1) & p(s_2|s_1,a_1) & \\dots & p(s_N|s_1,a_1)\n",
    "            \\end{bmatrix} = \n",
    "            \\begin{bmatrix}\n",
    "                0 & 1 & \\dots & 0\n",
    "            \\end{bmatrix} $$\n",
    "    $$ P(s_1,a_2) = \\begin{bmatrix}\n",
    "                p(s_1|s_1,a_1) & p(s_2|s_1,a_1) & \\dots & p(s_N|s_1,a_1)\n",
    "            \\end{bmatrix} = \n",
    "            \\begin{bmatrix}\n",
    "                1 & 0 & \\dots & 0\n",
    "            \\end{bmatrix} $$ $$ \\dots $$\n",
    "\n",
    "\n",
    "A sample deterministic policy looks like: $\\overbrace{a_1, a_1, a_2, a_3, ...}^\\text{|S|}$\n",
    "\n",
    "Let $|A|$ denote the number of actions, $|S|$ denote the number of states, then there are $|A|^{|S|}$ deterministic policies in total. (How to get that number: in each state, there are $|A|$ we can take, so there are $\\overbrace{|A|*|A|*...}^\\text{|S|} = |A|^{|S|}$ deterministic policies)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition\n",
    "---------\n",
    "Again let's define MDP with policy more formally: \n",
    "\n",
    "|MDP with policy|\n",
    "|:-------------|\n",
    "|$S$ is a set of states $s_t \\in S$|\n",
    "|$A$ is a set of actions $a \\in A $|\n",
    "|$P^{\\pi}$ is dynamics/transition model that specifies $p^{\\pi}(s'|{s})$ **under a certain policy**|    \n",
    "|$R^{\\pi}$ is a reward function that specifies current reward $R^{\\pi}(s)$ **under a certain policy**|\n",
    "|$\\gamma$ is the discount factor that $\\in [0,1]$|\n",
    "\n",
    "$p^{\\pi}(s'|{s})$ and $R^{\\pi}(s)$ are not defined directly in the environment, but we can construct it as follow:\n",
    "$$ p^{\\pi}(s'|s) = \\sum_{a \\in A} \\pi(a|s)p(s'|s,a) \\\\\n",
    "R^{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s)R(s,a) $$\n",
    "For example, using the deternimistic example from above, \n",
    "$$p^{\\pi}(s'|s_1) = \\pi(a_1|s_1) P(s_1,a_1) + \\pi(a_2|s_1) P(s_1,a_2) + ... \\\\\n",
    "= 1 * \\begin{bmatrix} 0 & 1 & \\dots & 0 \\end{bmatrix} + 0 * \\begin{bmatrix} 1 & 0 & \\dots & 0 \\end{bmatrix} \\\\\n",
    "= \\begin{bmatrix} 0 & 1 & \\dots & 0 \\end{bmatrix} $$\n",
    "\n",
    "This is actually the same as $p(s'|s,a_{\\pi})$, where $a_{\\pi}$ is action specified by $\\pi$ for state $s$.\n",
    "\n",
    "\n",
    "$$R^{\\pi}(s_1) = \\pi(a_1|s_1)R(s_1,a_1) + \\pi(a_2|s_1)R(s_1,a_2) + ... \\\\\n",
    "= R(s_1,a_1) $$      \n",
    "\n",
    "This is actually the same as $R(s,a_{\\pi})$, where $a_{\\pi}$ is action specified by $\\pi$ for state $s$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value function\n",
    "---------\n",
    "If we compare this with the definition of MRP, you will find it exactly the same as MRP. **$\\Rightarrow$ MDP + policy = MRP**. **So we can compute value exactly the way same as MRP.** In practise, we often use the **iterative** way:\n",
    "- for all $s \\in S$\n",
    "- initiate $V_0^{\\pi}(s) = 0$ \n",
    "- start with k = 1 until converge: $$ V_k^{\\pi}(s) = R^{\\pi}(s) + \\gamma \\sum_{s' \\in S} p^{\\pi}(s'|s)V_{k-1}^{\\pi}(s') $$ \n",
    "    \n",
    "using the same deternimistic example from above: \n",
    "    $$V_k^{\\pi}(s_1) = R^{\\pi}(s_1) + \\gamma \\begin{bmatrix} p^{\\pi}(s_1|s_1) & p^{\\pi}(s_2|s_1) & p^{\\pi}(s_3|s_1) & \\dots  & p^{\\pi}(s_N|s_1) \\end{bmatrix} \n",
    "            \\begin{bmatrix}\n",
    "                V_{k-1}^{\\pi}(s_1)  \\\\\n",
    "                V_{k-1}^{\\pi}(s_2)  \\\\\n",
    "                \\vdots \\\\\n",
    "                V_{k-1}^{\\pi}(s_N) \n",
    "            \\end{bmatrix} \\\\\n",
    "                    = R^{\\pi}(s_1) + \\gamma \\begin{bmatrix} 0 & 1 & \\dots & 0 \\end{bmatrix} \\begin{bmatrix}\n",
    "                V_{k-1}^{\\pi}(s_1)  \\\\\n",
    "                V_{k-1}^{\\pi}(s_2)  \\\\\n",
    "                \\vdots \\\\\n",
    "                V_{k-1}^{\\pi}(s_N) \n",
    "            \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Search \n",
    "-----------\n",
    "Finally, we can find the optimal policy that miaximises the value function, i.e. $$\\pi^*(s) = argmax_{\\pi}V^{\\pi}(s) $$\n",
    "\n",
    "There exists a unique optimal value function (but not neccassarily unique optimal policy). Optimal policy for a MDP in infinite time horizon is deterministic and stationary. We can do an exhaustive search on policy and compare the value or we can improve an initial policy until its value function converges. There are 2 ways of doing the later method: **policy iteration** (pi) and **value iteration** (vi).\n",
    "\n",
    "Let's first introduce a new function, **state-action value (Q value)**: \n",
    "\n",
    "$$Q^{\\pi}(s,a) = R(s,a) + \\gamma \\sum_{s' \\in S}P(s'|s,a)V^{\\pi}(s'), \\forall a \\in A$$\n",
    "\n",
    "It's just slightly different to the **state value** that we've seen earlier, $V^{\\pi}(s) = R^{\\pi}(s) + \\gamma \\sum_{s' \\in S}P^{\\pi}(s'|s)V^{\\pi}(s')$ \n",
    "\n",
    "#### 1. Policy Iteration\n",
    "Policy iteration involves 3 steps:\n",
    "- policy valuation, which is to compute $V^{\\pi}(s)$\n",
    "- policy improvement, where we take actions that maximise the Q value for each state, i.e. $$ \\pi_{i+1}(s) = argmax_a Q^{\\pi_i}(s,a)$$\n",
    "    - this will allow monotonically improve the policy since $Q^{\\pi_i}(s,a) \\geqslant Q^{\\pi_i}(s,\\pi_i(a))$\n",
    "- policy iteration: \n",
    "    - for all $s \\in S$, initialise $\\pi_0(s)$\n",
    "    - start with $i=0$ until $||\\pi_i - \\pi_{i+1}||_{l1} = 0$ (no changes in policy)\n",
    "        - policy valuation $V^{\\pi_i}(s)$\n",
    "        - policy imporvement $\\pi_{i+1}$\n",
    "        - i = i + 1\n",
    "\n",
    "#### 2. Value Iteration\n",
    "- for all $s \\in S$, initialise $V_0(s)$ with zeros \n",
    "- start with $k=1$ until $V(s)$ converges\n",
    "     - for each $s \\in S$: $V_{k+1}(s) = max_aQ(s,a)$\n",
    "     - k = k + 1\n",
    "- policy extraction: $\\pi_{k+1}(s) = argmax_aQ(s,a)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other MDPs\n",
    "-------------\n",
    "Depending on how agent state $s_t$ is defined, we have different types of MDP:\n",
    "- Full Observability / Markov Decision Process (MDP):\n",
    "    - Agent state uses observation from world state: $s_t = o_t$\n",
    "    - that is, we assume current observation is sufficient statistic of history, i.e. $o_t = h_t$ \n",
    "- Partial Observability / Partially Observable Markov Decision Process (POMDP):\n",
    "    - Agent state is not the same as the world state\n",
    "    - Agent contructs its own state: use $s_t = h_t$, or RNN, or beliefs of world state ...\n",
    "    - when?: when we don't have access to all information like in poker game\n",
    "- Bandits:\n",
    "    - actions have no influence on next observations (so actions don't affect real world) \n",
    "    - so actions are independent of one another \n",
    "    - there are no delayed rewards\n",
    "    - as opposed to bandits, MDP and POMDP actions influence next observation (so credict assignment and strategic actions might required)\n",
    "- Continuous-state MDP:\n",
    "    - requires optimal control\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy-Based\n",
    "** To be completed\n",
    "\n",
    "## Value-Based\n",
    "** To be completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlsa",
   "language": "python",
   "name": "mlsa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
