{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop RL01: Introduction to Reinforcement Learning\n",
    "\n",
    "## Motivation:\n",
    "\n",
    "So far we hace learned supervised learning, unsupervised learning as well as deep learning. It's probably a good time to stop and think about what is the fundamental challenge of machine learning and artificial intelligence. Quoting from reinforcement learning(RL) professor Emma Brunskill from Standford: \"Fundamental challenge in artificial intelligence and machine learning is \n",
    "\n",
    "\n",
    "**<center>learning to make good decisions under uncertainty\".</center>**\n",
    "\n",
    "\n",
    "If we break down this sentence into pieces, we can see that we need to address these following aspects:\n",
    "- \"learning\": no advanced knowledge, have to learn from experience\n",
    "- \"good decisions\": need some sort of measurement for decision-making process and optimize that measurement \n",
    "- \"uncertainty\": need to explore different probabilities to gain experience \n",
    "\n",
    "And RL is all about making **sequential decisions under uncertainty**, which involves:  \n",
    "\n",
    "- **optimization**: yield best desicions\n",
    "- **generalization**: generalise experience for decision-making in unprecedented situations  \n",
    "- **delayed consuquence**: account for decisions made now that can impact things much later \n",
    "- **exploration**: interact with the world through decision-making and learn what's the best decision  \n",
    "\n",
    "As a comparison with other AI methods:\n",
    "\n",
    "|Comparison|AI planning|Supervised ML|Unsupervised ML|Imitation learning| \n",
    "|:------:|:---------:|:-----------:|:-------------:|:----------------:|\n",
    "|optimization| $\\checkmark$ | $\\checkmark$ |$\\checkmark$| $\\checkmark$| \n",
    "|generalization|$\\checkmark$ |$\\checkmark$ |$\\checkmark$ |$\\checkmark$ |\n",
    "|delayed consuquence|$\\checkmark$ | - | - |$\\checkmark$ |\n",
    "|exploration| - | - | - | - |\n",
    "|how it learns|learn from models of how decisions impact results|learn from experience/data|learn from experience/data|learn from experience from other intelligence like human|\n",
    "\n",
    "\n",
    "Some successful RL implementations: \n",
    "Gaming, Robotics, Healthcare, ML (NLP, CV) ...\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The fundamentals\n",
    "\n",
    "So how does RL make sequential decisions? The answer should be pretty obvious: through a loop: \n",
    "\n",
    "\n",
    "<img src = 'SDP.png'>\n",
    "\n",
    "This is known as **sequential decision process**, at each time step $t$:\n",
    "- **agent** uses data up to time $t$ and takes action $a_t$\n",
    "- **world** emits observation $o_t$ and reward $r_t$, received by agent\n",
    "- data are stored in **history**: $h_t = (a_1,o_1,r_1,...,a_t,o_t,r_t)$\n",
    "\n",
    "\n",
    "|Examples|Action|Observation|Reward|\n",
    "|:------:|:----:|:---------:|:----:|\n",
    "|web ad|choose web ad|view time|click on ad|\n",
    "|blood pressure control|exercise or medication|blood pressure|within healthy range|\n",
    "\n",
    "Our goal is to maximise total expected (why expected?) future rewards, which may require balancing immediate and long-term rewards, as well as strategic behaviour to achieve high rewards. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL terminologies:\n",
    "- **agent**: an intelligent subject that can make actions\n",
    "- **world**: the environment that the agent operates in, and produces observations and rewards accordingly \n",
    "- **state**: information state assumed to determine what happens next\n",
    "- **wrold state**: representation of how the world changes, often true state of world is unknown to agent and we model it with limited data (why?)\n",
    "- **agent state**: information agent uses to make decisions and evaluate its rewards, e.g. if it's in state $s_1$ then do action $a_1$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL components\n",
    "An RL algorithm often contains one of more of:\n",
    "- **model**: \n",
    "    - mathematical models of dynamics and rewards, agent's representation of how the world changes in response to agent's action, e.g.:\n",
    "        - transition/dynamics model that predicts: $p(s_{t+1} = s'|s_t=s,a_t=a)$, e.g.\n",
    "        $$\\begin{bmatrix}\n",
    "            p(s_1|s_1,a_1) & p(s_2|s_1,a_1) & p(s_3|s_1,a_1) & \\dots  & p(s_N|s_1,a_1) \\\\\n",
    "            p(s_1|s_2,a_1) & p(s_2|s_2,a_1) & p(s_3|s_2,a_1) & \\dots  & p(s_N|s_2,a_1) \\\\\n",
    "            \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "            p(s_1|s_N,a_1) & p(s_2|s_N,a_1) & p(s_3|s_N,a_1) & \\dots  & p(s_N|s_N,a_1)\n",
    "        \\end{bmatrix}$$\n",
    "        - reward model that determines current rewards based on action and/or states: $R(s_t=s,a_t=a)=E \\lbrack r_t|s_t,a_t \\rbrack$\n",
    "    - explicit model, may or may not have policy \n",
    "- **policy**: \n",
    "    - function mapping agent's states to actions, determines agent's actions by some function $\\pi$, e.g.:\n",
    "        - deterministic policy: $a = \\pi(s)$\n",
    "        - stochastic policy: $p(a_t=a|s_t=s)=\\pi(a|s)$\n",
    "- **value function**: \n",
    "    - expected (discounted) future rewards, 2 types of value: \n",
    "        - state value: $V(s_t=s)=E\\lbrack r_t+\\gamma r_{t+1}+\\gamma^2r_{t+2}+...|s_t=s \\rbrack$\n",
    "        - state-action value, $Q(s_t=s,a_t=a)=E\\lbrack r_t+\\gamma r_{t+1}+\\gamma^2r_{t+2}+...|s_t=s , a_t = a\\rbrack$, where\n",
    "        - $\\gamma$ is the discount factor ($\\gamma \\in [0,1]$) \n",
    "    - used as a measurement of rewards for agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "By choosing and combining these components, we have different types of agents:\n",
    "\n",
    "<img src='agents.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value-Based\n",
    "In the value-based method, instead of calculating the value function explicitly, we want to approxiamte it with a set of parameters, i.e. $$V(s) \\approx V(s,w)$$ If we define $w$ like we did in deep neural network, then we have a Deep Q-learning Network (DQN), but more about this in the next workshop. \n",
    "\n",
    "\n",
    "### Policy-Based\n",
    "In the policy-based method, we parameterize and \"learn\" the poliy, i.e. $$\\pi(a|s) \\approx \\pi_{\\theta}(a|s)$$\n",
    "Then find the policy that maximize value function. More about this in the later policy gradient workshop. \n",
    "\n",
    "\n",
    "### Model-Based\n",
    "In the model-based method, we provide the agent a model how the world works. More explicitly, we define a dynamics/transition model that tells the agent how the state would change with its action. If we assume the states are Markov, we can define the agent's decision making process as a **Markov Decision Process (MDP)**. For those of you who are insterested, feel free to go through the optional workshop on more details about it. \n",
    "\n",
    "MDP often combines with policy, and is defined as below: \n",
    "\n",
    "|MDP with policy|\n",
    "|:-------------|\n",
    "|$S$ is a set of states $s_t \\in S$|\n",
    "|$A$ is a set of actions $a \\in A $|\n",
    "|$P^{\\pi}$ is dynamics/transition model that specifies $p^{\\pi}(s'|{s})$ **under a certain policy**, where $p^{\\pi}(s'|s) = \\sum_{a \\in A} \\pi(a|s)p(s'|s,a)$|    \n",
    "|$R^{\\pi}$ is a reward function that specifies current reward $R(s)$ **under a certain policy**, where $R^{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s)R(s,a)$|        \n",
    "|$\\gamma$ is the discount factor that $\\in [0,1]$|\n",
    "\n",
    "Then we can calculate the value function for policy in an iterative way:\n",
    "- for all $s \\in S$\n",
    "- initiate $V_0^{\\pi}(s) = 0$ \n",
    "- start with k = 1 until converge: $$ V_k^{\\pi}(s) = R^{\\pi}(s) + \\gamma \\sum_{s' \\in S} p^{\\pi}(s'|s)V_{k-1}^{\\pi}(s') $$ \n",
    "\n",
    "Using the above value function, we can also compute state-action value (Q value):\n",
    "$$Q^{\\pi}(s,a) = R(s,a) + \\gamma \\sum_{s' \\in S}P(s'|s,a)V^{\\pi}(s'), \\forall a \\in A$$\n",
    "\n",
    "And finaly, to find the optimal policy we can do an exhaustic search for policies and find the optimal one. Or we can improve the policy/value function iteratively to find the optimal one. (Which one is better?)\n",
    "\n",
    "The second method can be done in 2 ways:\n",
    "- policy iteration (pi)\n",
    "- value iteration (vi)\n",
    "\n",
    "#### 1. Policy Iteration\n",
    "Policy iteration involves 3 steps:\n",
    "- policy valuation, which is to compute $V^{\\pi}(s)$\n",
    "- policy improvement, where we take actions that maximise the Q value for each state, i.e. $$ \\pi_{i+1}(s) = argmax_a Q^{\\pi_i}(s,a)$$\n",
    "- policy iteration: \n",
    "    - for all $s \\in S$, initialise $\\pi_0(s)$\n",
    "    - start with $i=0$ until $||\\pi_i - \\pi_{i+1}||_{l1} = 0$ (no changes in policy)\n",
    "        - policy valuation $V^{\\pi_i}(s)$\n",
    "        - policy imporvement $\\pi_{i+1}$\n",
    "        - i = i + 1\n",
    "\n",
    "#### 2. Value Iteration\n",
    "- for all $s \\in S$, initialise $V_0(s)$ with zeros \n",
    "- start with $k=1$ until $V(s)$ converges\n",
    "     - for each $s \\in S$: $V_{k+1}(s) = max_aQ(s,a)$\n",
    "     - k = k + 1\n",
    "- policy extraction: $\\pi_{k+1}(s) = argmax_aQ(s,a)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "#### Task\n",
    "For the coding part, we're going to do the Standford reinforcement learning course assignment.\n",
    "([link](http://web.stanford.edu/class/cs234/assignment1/index.html))\n",
    "\n",
    "Your task is to implement policy iteration and value iteration in the frozen lake environment. Since it's hard to render in jupyter notebook, we'll be running scripts. \n",
    "\n",
    "What are the scripts:\n",
    "- discrete_env.py: dependency of frozen_lake.py\n",
    "- draft.py: prints out environment parameters to help understanding. You can also try printing out different parameters.\n",
    "- frozen_lake.py: creates environment; specifies states, actions and dynamics model. (More info [here](https://gym.openai.com/envs/FrozenLake-v0/))\n",
    "- getting_started_with_gym.py: simple example to help you familiarise yourself with frozen lake environment. You can also try other environment in the gym. \n",
    "- lake_env.py: defines and registers for specific frozen lake environments\n",
    "- vi_and_pi.py: implementation of value iteration and policy iteration. We have provided the suggested solutions but feel free to try your own solutions!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up\n",
    "To set up the world or environment, we're going to use \"gym\". (Doc [here](https://gym.openai.com/docs/))\n",
    "\n",
    "\n",
    "Gym contains lots of common environments (Pacman, cartpole etc.) that we can try. Go to [full list](https://gym.openai.com/envs/#classic_control) to check out more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym==0.10.9 (from -r ./assignment1/requirements.txt (line 1))\n",
      "Requirement already satisfied: matplotlib in /anaconda3/lib/python3.7/site-packages (from -r ./assignment1/requirements.txt (line 3)) (3.0.3)\n",
      "Requirement already satisfied: numpy in /anaconda3/lib/python3.7/site-packages (from -r ./assignment1/requirements.txt (line 4)) (1.16.2)\n",
      "Requirement already satisfied: scipy in /anaconda3/lib/python3.7/site-packages (from -r ./assignment1/requirements.txt (line 5)) (1.2.1)\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.7/site-packages (from gym==0.10.9->-r ./assignment1/requirements.txt (line 1)) (1.12.0)\n",
      "Requirement already satisfied: requests>=2.0 in /anaconda3/lib/python3.7/site-packages (from gym==0.10.9->-r ./assignment1/requirements.txt (line 1)) (2.21.0)\n",
      "Collecting pyglet>=1.2.0 (from gym==0.10.9->-r ./assignment1/requirements.txt (line 1))\n",
      "  Using cached https://files.pythonhosted.org/packages/6b/aa/121ad16b96b6141a04d781be38215581162031bb0410ccf15fc9a597e02f/pyglet-1.4.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: cycler>=0.10 in /anaconda3/lib/python3.7/site-packages (from matplotlib->-r ./assignment1/requirements.txt (line 3)) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /anaconda3/lib/python3.7/site-packages (from matplotlib->-r ./assignment1/requirements.txt (line 3)) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /anaconda3/lib/python3.7/site-packages (from matplotlib->-r ./assignment1/requirements.txt (line 3)) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /anaconda3/lib/python3.7/site-packages (from matplotlib->-r ./assignment1/requirements.txt (line 3)) (2.8.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.7/site-packages (from requests>=2.0->gym==0.10.9->-r ./assignment1/requirements.txt (line 1)) (2019.3.9)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /anaconda3/lib/python3.7/site-packages (from requests>=2.0->gym==0.10.9->-r ./assignment1/requirements.txt (line 1)) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /anaconda3/lib/python3.7/site-packages (from requests>=2.0->gym==0.10.9->-r ./assignment1/requirements.txt (line 1)) (1.24.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.7/site-packages (from requests>=2.0->gym==0.10.9->-r ./assignment1/requirements.txt (line 1)) (3.0.4)\n",
      "Requirement already satisfied: future in /anaconda3/lib/python3.7/site-packages (from pyglet>=1.2.0->gym==0.10.9->-r ./assignment1/requirements.txt (line 1)) (0.17.1)\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->-r ./assignment1/requirements.txt (line 3)) (40.8.0)\n",
      "Installing collected packages: pyglet, gym\n",
      "Successfully installed gym-0.10.9 pyglet-1.4.4\n"
     ]
    }
   ],
   "source": [
    "! pip install -r ./assignment1/requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We realise that over the 3 workshops we can only cover the very basis of RL and the most common used algorithms (DQN, policy gradient). If you are really interested, check out [this **open-source** Standford course](http://web.stanford.edu/class/cs234/schedule.html). \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlsa",
   "language": "python",
   "name": "mlsa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
