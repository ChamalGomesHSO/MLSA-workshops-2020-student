{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop RL01: Introduction to Reinforcement Learning\n",
    "\n",
    "## Motivation:\n",
    "\n",
    "So far we hace learned supervised learning, unsupervised learning as well as deep learning. It's probably a good time to stop and think about what is the fundamental challenge of machine learning and artificial intelligence. Quoting from reinforcement learning(RL) professor Emma Brunskill from Standford: \"Fundamental challenge in artificial intelligence and machine learning is \n",
    "\n",
    "\n",
    "**<center>learning to make good decisions under uncertainty\".</center>**\n",
    "\n",
    "\n",
    "If we break down this sentence into pieces, we can see that we need to address these following aspects:\n",
    "- \"learning\": no advanced knowledge, have to learn from experience\n",
    "- \"good decisions\": need some sort of measurement for decision-making process and optimize that measurement \n",
    "- \"uncertainty\": need to explore different probabilities to gain experience \n",
    "\n",
    "And RL is all about making **sequential decisions under uncertainty**, which involves:  \n",
    "\n",
    "- **optimization**: yield best desicions\n",
    "- **generalization**: generalise experience for decision-making in unprecedented situations  \n",
    "- **delayed consuquence**: account for decisions made now that can impact things much later \n",
    "- **exploration**: interact with the world through decision-making and learn what's the best decision  \n",
    "\n",
    "As a comparison with other AI methods:\n",
    "\n",
    "|Comparison|AI planning|Supervised ML|Unsupervised ML|Imitation learning| \n",
    "|:------:|:---------:|:-----------:|:-------------:|:----------------:|\n",
    "|optimization| $\\checkmark$ | $\\checkmark$ |$\\checkmark$| $\\checkmark$| \n",
    "|generalization|$\\checkmark$ |$\\checkmark$ |$\\checkmark$ |$\\checkmark$ |\n",
    "|delayed consuquence|$\\checkmark$ | - | - |$\\checkmark$ |\n",
    "|exploration| - | - | - | - |\n",
    "|how it learns|learn from models of how decisions impact results|learn from experience/data|learn from experience/data|learn from experience from other intelligence like human|\n",
    "\n",
    "\n",
    "Some successful RL implementations: \n",
    "Gaming, Robotics, Healthcare, ML (NLP, CV) ...\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The fundamentals\n",
    "\n",
    "So how does RL make sequential decisions? The answer should be pretty obvious: through a loop: \n",
    "\n",
    "\n",
    "<img src = 'SDP.png'>\n",
    "\n",
    "This is known as **sequential decision process**, at each time step $t$:\n",
    "- **agent** uses data up to time $t$ and takes action $a_t$\n",
    "- **world** emits observation $o_t$ and reward $r_t$, received by agent\n",
    "- data are stored in **history**: $h_t = (a_1,o_1,r_1,...,a_t,o_t,r_t)$\n",
    "\n",
    "\n",
    "|Examples|Action|Observation|Reward|\n",
    "|:------:|:----:|:---------:|:----:|\n",
    "|web ad|choose web ad|view time|click on ad|\n",
    "|blood pressure control|exercise or medication|blood pressure|within healthy range|\n",
    "\n",
    "Our goal is to maximise total expected (why expected?) future rewards, which may require balancing immediate and long-term rewards, as well as strategic behaviour to achieve high rewards. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The terminologies:\n",
    "- **agent**: an intelligent subject that can make actions\n",
    "- **world**: the environment that the agent operates in, and produces observations and rewards accordingly \n",
    "- **state**: information state assumed to determine what happens next\n",
    "- **wrold state**: representation of how the world changes, often true state of world is unknown to agent and we model it with limited data (why?)\n",
    "- **agent state**: information agent uses to make decisions, generally some function of history, i.e. $s_t = f(h_t)$, could also include meta data like how many computations executed and how many decisions left "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL components\n",
    "An RL algorithm often contains one of more of:\n",
    "- **model**: mathematical models of dynamics and rewards\n",
    "    - agent's representation of how the world changes in response to agent's action, e.g.:\n",
    "    - transition/dynamics model that predicts $p(s_{t+1}|s_t,a_t)$\n",
    "    - reward model that determines rewards based on action and/or states $R(s_t=s,a_t=a)=E \\lbrack r_t|s_t,a_t \\rbrack$\n",
    "    - explicit model, may or may not have policy and/or value function\n",
    "- **policy**: functions mapping agent's states to actions \n",
    "    - determines agent's actions by some function $\\pi$, e.g.:\n",
    "    - deterministic policy: $a = \\pi(s)$\n",
    "    - stochastic policy: $p(a_t=a|s_t=s)=\\pi(a|s)$\n",
    "- **value function**: expected (discounted) future rewards:\n",
    "    - if we start in state $s$, value function is defined as:\n",
    "    - $V(s_t=s)=E\\lbrack r_t+\\gamma r_{t+1}+\\gamma^2r_{t+2}+...|s_t=s \\rbrack$, where\n",
    "    - $\\gamma$ is the discount factor (if $\\gamma<1$ we place more weights to recent rewards) \n",
    "    - can be used to quantify goodness and badness of states and actions\n",
    "    - can compare different policies\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "By choosing and combining these components, we have different types of agents:\n",
    "\n",
    "<img src='agents.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the coding part, we're going to use the Standford reinforcement course assignment\n",
    "# link: http://web.stanford.edu/class/cs234/assignment1/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We realised that over the 3 workshops we can only cover the very basis of RL and the most common used algorithms (DQN, policy gradient). If you are really interested, dare yourself and try this [this **open-source** Standford course](http://web.stanford.edu/class/cs234/schedule.html). \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
