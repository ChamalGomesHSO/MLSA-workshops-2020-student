{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop RL02: value-based RL (Deep Q-learning focus)\n",
    "## Motivation:\n",
    "In the last workshop for model-based RL, we have seen how we can model states transitions/dynamics by defining a transition probabilities matrix. Then we use that to calculate the value functions iteratively. But what if we have enormous states and/or actions, or transition probabilities are hard to define (e.g. self-driving, Atari, consumer marketing, healthcare etc.).  Instead of finding the value functions explicitly, we can estimate with a parameterized function, e.g.\n",
    "\n",
    "<img src='param-est.png' width=400>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Function Approximation (VFA)\n",
    "There are many ways of approximations: \n",
    "- linear approximation: $\\hat{V}^{\\pi}(s;w) = x(s)^Tw$\n",
    "- Decision trees\n",
    "- Nearest neighbours\n",
    "- Neural Netwrok\n",
    "\n",
    "Our focus here will be on neural netwrok. An example of neural network approximations:\n",
    "<img src='DQN.png'>\n",
    "- Input $s$ is a stack of raw pixels from last 4 frames (that represents the current state)\n",
    "- Reward is change in score for that step \n",
    "- Output is the approximation of $Q(s,a)$ values for all actions (e.g. joystick/buttons) for the current state.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters updating\n",
    "Then our goal is to find the set of parameters $W$ that minimize the loss between true value $Q^{\\pi}(s,a)$ and its approximations $\\hat{Q}^{\\pi}(s,a;w)$. \n",
    "\n",
    "To bridge the gap between true values and approximations, first, let's define the **loss function**:\n",
    "$$ J(w) = E_{\\pi} \\lbrack (Q^{\\pi}(s,a)-\\hat{Q^{\\pi}}(s,a;w))^2 \\rbrack $$\n",
    "\n",
    "\n",
    "Then using **gradient descent** to find the **local minimum**:\n",
    "$$ \\Delta w = -\\frac{\\alpha}{2}\\nabla_wJ(w) $$\n",
    "> note: $\\Delta w$ is the updates of $w$, $\\nabla_w$ is the derivative with respect to $w$. \n",
    "\n",
    "Now plugging in $\\nabla_wJ(w)$: \n",
    "$$ \\Delta w = \\alpha E_{\\pi} \\lbrack (Q^\\pi(s,a)-\\hat{Q^{\\pi}}(s,a;w))\\nabla_w\\hat{Q^{\\pi}}(s,a;w) \\rbrack $$\n",
    "\n",
    "\n",
    "Finally, since this is still expection value, we can estimate using **stochastic gradient descent**:\n",
    "$$ \\Delta w = \\alpha (Q^\\pi(s,a)-\\hat{Q^{\\pi}}(s,a;w))\\nabla_w\\hat{Q^{\\pi}}(s,a;w) $$ \n",
    "> i.e. compute this using a single random sample. If we do this enough of times, we can converge to the result of gradient descent (expected SGD is the same as the full gradient descent). \n",
    "\n",
    ">$Q^\\pi(s,a)-\\hat{Q^{\\pi}}(s,a;w)$ is also called **prediction error**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximation with an Oracle\n",
    "Notice that we still don't have the true value for value functions $Q^{\\pi}(s,a)$. We can overcome this by using some sort of **oracle** to tell the true value, e.g.:\n",
    "- Monte Carlo method: generate an episode and use its return $G_t = \\sum_t r_t \\gamma ^t$ as subsitute $$\\Delta w = \\alpha (G_t-\\hat{Q_t}(s_t,a_t;w))\\nabla_w\\hat{Q_t}(s_t,a_t;w)$$\n",
    "> unbiased, high variance\n",
    "- SARSA (State-Action-Reward-State-Action) method: use a Temporal Difference (TD) target that leverages the current function approximation value\n",
    "$$\\Delta w = \\alpha (r+\\gamma \\hat{Q}(s_{t+1},a_{t+1};w)-\\hat{Q_t}(s_t,a_t;w))\\nabla_w\\hat{Q_t}(s_t,a_t;w)$$\n",
    "- Q-learning method: use a TD target that leverages the maximum of the current function approximation value \n",
    "$$\\Delta w = \\alpha (r+\\gamma max_a \\hat{Q}(s_{t+1},a;w)-\\hat{Q_t}(s_t,a_t;w))\\nabla_w\\hat{Q_t}(s_t,a_t;w)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQNs in practise\n",
    "Our focus here is Deep Q-learning Network (DQN), which uses deep CNN as approximiation. Let's discuss in more details about some methods that can improve performance of DQNs. \n",
    "\n",
    "### Better Convergence\n",
    "DQN using VFA can diverge, because:\n",
    "- correlation between samples (iid assumption for SGD convergence)\n",
    "- non-stationary targets (weights keep changing, and policy keeps changing)\n",
    "\n",
    "DQN can address these problems by: 1. Experience Replay and 2. Fixed Q-targets\n",
    "\n",
    "#### 1. Experience Replay\n",
    "This is to remove correlation between samples. Idea is to sample from experience to cancel out the correlation between sample and preceding one, e.g. $s_t, s_{t+1}$.\n",
    "\n",
    "First store data from prior experience, i.e. **Replay Buffer** $D$.\n",
    "\n",
    "|Replay Buffer|\n",
    "|:-----------:|\n",
    "|$s_1,a_1,r_1,s_2$|\n",
    "|$s_2,a_2,r_2,s_3$|\n",
    "|$s_3,a_3,r_3,s_4$|\n",
    "| ... |\n",
    "|$s_t,a_t,r_t,s_{t+1}$|\n",
    "\n",
    "> note: underscore here means timestep\n",
    "\n",
    "Then sample a minibatch of $(s,a,r,s')$ from the buffer $D$ and update the weights using SGD as follow:\n",
    "$$\\Delta w = \\alpha (r+\\gamma max_{a'} \\hat{Q}(s',a';w)-\\hat{Q}(s,a;w))\\nabla_w\\hat{Q}(s,a;w)$$\n",
    "\n",
    "#### 2. Fixed Q-targets\n",
    "This is to improve stability of weights so as to avoid them from exploding. Idea is to fix the target for multiple updates to stop weights from updating drastically. \n",
    "\n",
    "First we define 2 different sets of weights. \n",
    "- $w^-$ be the set of weights for fixing the targets (fixed for some updates)\n",
    "- $w$ be the set of weights that are being updated \n",
    "\n",
    "Then take an sample from replay buffer $D$: $(s,a,r,s')$ and update the weights as follow:\n",
    "$$\\Delta w = \\alpha (r+\\gamma max_{a'} \\hat{Q}(s',a';w^-)-\\hat{Q}(s,a;w))\\nabla_w\\hat{Q}(s,a;w)$$\n",
    "\n",
    "### Exploration or Exploitation\n",
    "**Exploitation** means the agent uses the current information to make the best decisions whereas **exploration** means the agent explores other actions that might lead to better results. \n",
    "\n",
    "We want to do both but we also want to find the fine balance between these 2. We can achieve this by **$\\varepsilon$-greedy**: \n",
    "$$\\pi(a|s)=\\bigg\\{  \\begin{array}{ll}\n",
    "                    argmax_aQ(s,a),\\ with\\ prob.\\ 1-\\varepsilon \\\\ \n",
    "                    a_1, a_2, ... ,\\ with\\ prob.\\ \\frac{\\varepsilon}{|A|} \n",
    "                    \\end{array}$$\n",
    "> how do we choose $\\varepsilon$? DeepMind's paper (mnih2015human, mnih-atari-2013), decreases $\\varepsilon$ from 1 to 0.1 during the first million steps. But in test time, $\\varepsilon_{soft} = 0.05$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other better-performing DQNs, which are included in the optional workshop. Feel free to go through it and try implementing them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "### Task\n",
    "In the exercise we will be doing the [assignment from Standford Reinforcement Learning course](http://web.stanford.edu/class/cs234/assignment2/index.html) again. This time we will train a DQN (feel free to implement other improved DQNs as well) in [Atari 2600 game Pong](https://gym.openai.com/envs/Pong-v0/) to play against another decent AI. Since this will take **12 hours** to train on a GPU, so we will first test it on a testing environment then later you can train it on your own free time if you're interested. \n",
    "\n",
    "### Settings\n",
    "These are basically what we have talked about in the above section, but we just put them here for your convinience.\n",
    "- tabular setting: \n",
    "    In the tabular setting, we want to matintain a table for $(s_t,a_t,r_t,s_{t+1},Q(s_t,a_t))$, and update $Q$ as follow\n",
    "    $$Q(s_t,a_t;w) \\leftarrow Q(s_t,a_t;w)+\\alpha \\big( r_t + \\gamma \\max\\limits_{a' \\in A} Q(s_{t+1},a';w) - Q(s_t,a_t) \\big) $$\n",
    "    \n",
    "- approximation setting:\n",
    "    In the approximation setting, instead of storing $Q(s,a)$ itself, we represent $Q(s_t,a_t)$ as $\\hat{Q}(s_t,a_t;w)$. (So we just need a set of $w$ for all $\\hat{Q}$) Then we update the weights $w$ as follow\n",
    "$$w \\leftarrow w + \\alpha (r_t+\\gamma max_{a' \\in A} \\hat{Q}(s_{t+1},a';w)-\\hat{Q}(s_t,a_t;w))\\nabla_w\\hat{Q}(s_t,a_t;w)$$\n",
    "\n",
    "- target network:\n",
    "    In the target network setting we maintain two sets of weights for updating ($w$) and fixing target ($w^-$)   \n",
    "    $$w \\leftarrow w + \\alpha (r_t+\\gamma max_{a' \\in A} \\hat{Q}(s_{t+1},a';w^-)-\\hat{Q}(s_t,a_t;w))\\nabla_w\\hat{Q}(s_t,a_t;w)$$\n",
    "\n",
    "- replay memory:\n",
    "    In the replay memory setting, we store the tuple $(s_t,a_t,r_t,s_{t+1})$ in a buffer, and sample a minibatch from the buffer to update weights.\n",
    "    \n",
    "- $\\varepsilon$-greedy:\n",
    "    In $\\varepsilon$-greedy setting, we decrease $\\varepsilon$ from 1 to 0.1 during the first million steps, and during test time we use $\\varepsilon_{soft} = 0.05$. \n",
    "\n",
    "### Setting up the pipeline\n",
    "To set up the whole pipeline of RL, we need to do the following in these scritps:\n",
    "- set up learning rate schedule and $\\varepsilon$-greedy in q1_schedule.py\n",
    "- implement linear approximation in q2_linear.py\n",
    "- implement DQN as descirbed in [mnih2015human](https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf) in q3_nature.py\n",
    "\n",
    "\n",
    "Follow carefully the instructions in the assignment and keep searching for keywords in Tensorflow documentation. Have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[atari] in /anaconda3/lib/python3.7/site-packages (from -r ./assignment2/requirements.txt (line 1)) (0.10.9)\n",
      "Collecting matplotlib==2.0.2 (from -r ./assignment2/requirements.txt (line 3))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/f0/9da3ef24ea7eb0ccd12430a261b66eca36b924aeef06e17147f9f9d7d310/matplotlib-2.0.2.tar.gz (53.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 53.9MB 800kB/s \n",
      "\u001b[?25h    Complete output from command python setup.py egg_info:\n",
      "    IMPORTANT WARNING:\n",
      "        pkg-config is not installed.\n",
      "        matplotlib may not be able to find some of its dependencies\n",
      "    ============================================================================\n",
      "    Edit setup.cfg to change the build options\n",
      "    \n",
      "    BUILDING MATPLOTLIB\n",
      "                matplotlib: yes [2.0.2]\n",
      "                    python: yes [3.7.3 (default, Mar 27 2019, 16:54:48)  [Clang\n",
      "                            4.0.1 (tags/RELEASE_401/final)]]\n",
      "                  platform: yes [darwin]\n",
      "    \n",
      "    REQUIRED DEPENDENCIES AND EXTENSIONS\n",
      "                     numpy: yes [version 1.16.2]\n",
      "                       six: yes [using six version 1.12.0]\n",
      "                  dateutil: yes [using dateutil version 2.8.0]\n",
      "               functools32: yes [Not required]\n",
      "              subprocess32: yes [Not required]\n",
      "                      pytz: yes [using pytz version 2018.9]\n",
      "                    cycler: yes [using cycler version 0.10.0]\n",
      "                   tornado: yes [using tornado version 6.0.2]\n",
      "                 pyparsing: yes [using pyparsing version 2.3.1]\n",
      "                    libagg: yes [pkg-config information for 'libagg' could not\n",
      "                            be found. Using local copy.]\n",
      "                  freetype: no  [The C/C++ header for freetype2 (ft2build.h)\n",
      "                            could not be found.  You may need to install the\n",
      "                            development package.]\n",
      "                       png: yes [version 1.6.36]\n",
      "                     qhull: yes [pkg-config information for 'qhull' could not be\n",
      "                            found. Using local copy.]\n",
      "    \n",
      "    OPTIONAL SUBPACKAGES\n",
      "               sample_data: yes [installing]\n",
      "                  toolkits: yes [installing]\n",
      "                     tests: no  [skipping due to configuration]\n",
      "            toolkits_tests: no  [skipping due to configuration]\n",
      "    \n",
      "    OPTIONAL BACKEND EXTENSIONS\n",
      "                    macosx: yes [installing, darwin]\n",
      "                    qt5agg: yes [installing, Qt: 5.9.6, PyQt: 5.9.6]\n",
      "                    qt4agg: no  [PySide not found; PyQt4 not found]\n",
      "                   gtk3agg: no  [Requires pygobject to be installed.]\n",
      "                 gtk3cairo: no  [Requires cairocffi or pycairo to be installed.]\n",
      "                    gtkagg: no  [Requires pygtk]\n",
      "                     tkagg: yes [installing; run-time loading from Python Tcl /\n",
      "                            Tk]\n",
      "                     wxagg: no  [requires wxPython]\n",
      "                       gtk: no  [Requires pygtk]\n",
      "                       agg: yes [installing]\n",
      "                     cairo: no  [cairocffi or pycairo not found]\n",
      "                 windowing: no  [Microsoft Windows only]\n",
      "    \n",
      "    OPTIONAL LATEX DEPENDENCIES\n",
      "                    dvipng: no\n",
      "               ghostscript: no\n",
      "                     latex: no\n",
      "                   pdftops: yes [version 0.80.0]\n",
      "    \n",
      "    OPTIONAL PACKAGE DATA\n",
      "                      dlls: no  [skipping due to configuration]\n",
      "    \n",
      "    ============================================================================\n",
      "                            * The following required packages can not be built:\n",
      "                            * freetype\n",
      "    \n",
      "    ----------------------------------------\n",
      "\u001b[31mCommand \"python setup.py egg_info\" failed with error code 1 in /private/var/folders/py/439h61mx5p32nqdjmyfhlgy80000gn/T/pip-install-p0ch29cp/matplotlib/\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -r ./assignment2/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlsa",
   "language": "python",
   "name": "mlsa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
