{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop RL03: policy-based RL (policy-gradient focus)\n",
    "\n",
    "## Motivation: \n",
    "So far we have learnt model-based and value-based RL, and now let's talk about the more common used method, namely the policy-based method. **Our goal is to parameterise policy $\\pi_\\theta(s,a)$ and find the parameters for the best policy that maximises value function.**\n",
    "\n",
    "Before we go to the details, let's look at the pros and cons of policy-based.\n",
    "\n",
    "**Pros:** \n",
    "- Better convergence properties\n",
    "- Effective in high-dimemesion or continuous action spaces\n",
    "    - value-based is effective in high-dimemsion states spaces but not in high-dimemsion action spaces\n",
    "- Can learn stochastic policies\n",
    "    - important for non-stationary and/or non-markov states \n",
    "    - e.g. a deterministic policy can easily be exploited in a game like rosk-paper-scissors\n",
    "    - e.g. the agent cannot differentiate the states that are far away from rewards (the grey states in the follwing picture)\n",
    "    <img src = 'non-markov.png' width=400>\n",
    "    \n",
    "\n",
    "**Cons:**\n",
    "- Typically converge to local optima rather than global optima \n",
    "    - i.e. good policy but not neccessarily the best\n",
    "- Evaluating policies is typically inefiicient and high variance \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to parameterise policy \n",
    "Let's start with how to parameterise our policy. There are many choices of differentiable parameterised policy: \n",
    "1. Softmax policy\n",
    "$$\\pi_\\theta(s,a) = \\frac{e^{\\phi(s,a)^T \\theta}}{\\sum_a e^{\\phi(s,a)^T \\theta}},$$\n",
    "where $\\phi(s,a)^T \\theta$ are weights assigned to actions using linear combination of feaures. \n",
    "2. Gaussian policy \n",
    "$$a \\sim N(\\mu(s),\\sigma^2),$$\n",
    "where $\\mu(s)$ is a linear combination of state features $\\mu(s)=\\phi(s)^T \\theta$, and $\\sigma$ is fixed ot could be parameterised.\n",
    "3. Neural networks policy \n",
    "    \n",
    "    Output layer is policy $\\pi(s,a)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient-free method\n",
    "To find the parameters of the optimal policy, there are gradient-based and gradient-free methods. Our focus is gradient-based, but you can also research in your own time and try the following gradient-free ways: (often a great simple baseline to try, can work with non-differentiables as well)\n",
    "- Hill climbing \n",
    "- Simplex/amoeba/Nelder Mead\n",
    "- Genetic algorithms\n",
    "- Cross-entropy method (CEM)\n",
    "- Covariance matrix adaptation (CMA)\n",
    "\n",
    "## Gradient-based method: Policy gradient \n",
    "To implement gradient-based method to find the best policy, we first need to define $V(\\theta)=V^{\\pi_\\theta}$ to make explicit the dependence of value on policy parameters. We also assume eposodic MDPs (Markov Decision Process). \n",
    "\n",
    "Since we're looking for parameters that **maximise** value function, we update the parameters by **gradient ascend**, i.e.\n",
    "\n",
    "$$\\Delta \\theta = \\alpha \\nabla_\\theta V(\\theta),$$\n",
    "where $\\nabla_\\theta V(\\theta)$ is the **policy gradient**:\n",
    "$$\\nabla_\\theta V(\\theta) = \n",
    "        \\begin{bmatrix}\n",
    "            \\frac{\\partial V(\\theta)}{\\partial \\theta _1} \\\\\n",
    "            \\frac{\\partial V(\\theta)}{\\partial \\theta _2}  \\\\\n",
    "            \\vdots  \\\\\n",
    "            \\frac{\\partial V(\\theta)}{\\partial \\theta _n} \n",
    "        \\end{bmatrix}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing policy gradient analytically \n",
    "Now the big problem is how to compute these gradients. We can estimate policy gradient by finite difference but that can be too simple and not efficient. So we will focus on a more common practise, which is to compute policy gradient analytically. There are many different ways: \n",
    "1. [Score function](https://en.wikipedia.org/wiki/Score_(statistics)) policy gradient \n",
    "2. Monte-Carlo policy gradient\n",
    "3. \"Vanilla\" policy gradient\n",
    "... and so on.\n",
    "\n",
    "Follwoing are the results from analytically computing policy gradient. The derivation is included in the optional workshop if you're interested.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Score function policy gradient \n",
    "$$\\nabla_\\theta V(\\theta) \\approx (1/m) \\sum_{i=1}^m R(\\tau^{(i)}) \\sum_{t=0}^{T-1} \\underbrace{\\nabla_\\theta \\log \\pi_\\theta(a_t^{(i)}|s_t^{(i)})}_{score\\ function} $$\n",
    "$$\\Delta \\theta = (\\alpha/m) \\sum_{i=1}^m R(\\tau^{(i)}) \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t^{(i)}|s_t^{(i)}) $$\n",
    "\n",
    "For a Gaussian policy, the score function is $\\nabla_\\theta \\log \\pi_\\theta(a_t^{(i)}|s_t^{(i)}) = \\frac{(a-\\mu(s))\\phi(s)}{\\sigma^2}$\n",
    "### 2. Monte-Carlo policy gradient (REINFORCE)\n",
    "Apart from using the above score funtion policy gradient, we can also leverage temporal structure (update parameters as the episode goes instead of waiting until the episode ends) and use Monte-Carlo policy gradient as follow:\n",
    "$$\\Delta \\theta = \\alpha \\nabla_\\theta \\log \\pi_\\theta(s_t,a_t) G_t, $$\n",
    "where $G_t$ is the Monte-Carlo return at time $t$.\n",
    "\n",
    "**REINFORCE** algorithm:\n",
    "- intialise a random policy with parameter $\\theta$\n",
    "- for each episode {$s_1,a_1,r_1,...,s_{T-1},a_{T-1},r_T$} $\\sim \\pi_\\theta$ do\n",
    "    - for $t=1$ to $T-1$ do \n",
    "        - $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\log \\pi_\\theta(s_t,a_t) G_t$\n",
    "- return $\\theta$\n",
    "\n",
    "### 3. \"Vanilla\" policy gradient\n",
    "We can also introduce a baseline $b(s_t)$ to reduce variance in parameters. The great thing about this is that for any choice of baseline, gradient estimator is unbiased.  \n",
    "\n",
    "**\"Vanilla\" policy gradient** algorithm:\n",
    "- initialise a policy with parameter $\\theta$ and a baseline $b$\n",
    "- for iteration = 1, 2, ... do \n",
    "    - collect a set of episodes by executing the current policy \n",
    "    - for each episode {$s_1,a_1,r_1,...,s_{T-1},a_{T-1},r_T$} $\\sim \\pi_\\theta$ do\n",
    "        - for $t=1$ to $T-1$ do\n",
    "            - the return $R_t = \\sum_{t'=t}^{T-1} r_{t'}$, and\n",
    "            - the advantage estimate $\\hat{A}_t = R_t - b(s_t)$\n",
    "    - re-fit the baseline by minimising $||b(s_t)-R_t||^2$ summed over all episodes and timesteps\n",
    "    - update the policy using policy gradient estimates $\\hat{g} = \\sum_t  \\nabla_\\theta \\log \\pi_\\theta(s_t,a_t) \\hat{A}_t$\n",
    "        - $\\theta \\leftarrow \\theta + \\hat{g} $\n",
    "        - (plug $\\sum_t \\nabla_\\theta \\log \\pi_\\theta(s_t,a_t) \\hat{A}_t$ into SGD or ADAM)\n",
    "- return $\\theta$\n",
    "\n",
    "Usually $\\sum_t  \\nabla_\\theta \\log \\pi_\\theta(s_t,a_t) \\hat{A}_t$ is inefficient, so we want to batch data and define a surrogate function using the current batch:\n",
    "$$L(\\theta) = \\sum_t \\log \\pi_\\theta(s_t,a_t) \\hat{A}_t$$\n",
    "Or we can include value function fit error: \n",
    "$$L(\\theta) = \\sum_t \\big( \\log \\pi_\\theta(s_t,a_t) \\hat{A}_t -||V(s_t)-\\hat{R}_t||^2 \\big)$$\n",
    "Then the policy gradient estimates become $$ \\hat{g} = \\nabla_\\theta L(\\theta)$$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise \n",
    "Again we will be using [Standford reinforcement learning course assignment](http://web.stanford.edu/class/cs234/assignment3/index.html) for exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlsa",
   "language": "python",
   "name": "mlsa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
