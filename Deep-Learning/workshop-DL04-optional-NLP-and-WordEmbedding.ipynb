{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop DL04: NLP and word embeddings** \n",
    "\n",
    "** To be completed\n",
    "\n",
    "## Motivation:\n",
    "In the [RNN workshop](https://github.com/Phoebe0222/MLSA-workshops-2019-student/blob/master/Deep-Learning/workshop-DL03-RecurrentNeuralNetworks.ipynb), before we feed the sequence to the RNN, we have used an extra layer called **word embedding**, which is a group pretrained word vectors known as [GloVe](https://nlp.stanford.edu/projects/glove/). This optional workshop is intended to show you how these word vectors are created, and its role in natural language processing.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings:\n",
    "\n",
    "Recall that we represent our words by tokenization, where we create a dictionary and assign a distint id (e.g. a number) to every word. For example, Man(5391), Woman(9853), Apple(456), Orange(6257) etc. Then in one-hot representation, it's represented as a vector with the dimension same as dictionary size (e.g. $n$), where the element in the index same as its id number being 1 and all other elements being 0, e.g. \n",
    "#### <center>One-hot Encoding</center>\n",
    "|o_man|index|\n",
    "|:-:|:---:|\n",
    "|$\\begin{pmatrix}0\\\\0\\\\0\\\\...\\\\1\\\\ ...\\\\0\\\\\\end{pmatrix}$|$\n",
    "\\begin{matrix}1\\\\2\\\\3\\\\...\\\\5391\\\\ ...\\\\n\\\\\\end{matrix}$|\n",
    "\n",
    "and similarily for other words.\n",
    "\n",
    "### Problem with one-hot encoding\n",
    "I guess you can see the problem now, that the id assigned to similar words are quite distant, without any implications about relationships of words. For example, it can't tell that the meaning of man to woman are more related than man to apple, or generalise popular phases like orange juice.   \n",
    "\n",
    "\n",
    "### Alternative to one-hot = word embedding\n",
    "What if we can use a matrix to represent the relationships between words and some predetermined features such as gender, fruit, color etc.? For example, gender is associated with man and woman but not apple or orange, so we assign $-1$ to man and $1$ to woman in that feature, and very small values to apple and orange. Similarily, depending on how related the words are to other features, we assign different values to words corresponding to each feature, like this:\n",
    "\n",
    "#### <center>Embeddings Matrix</center>\n",
    "|Features\\word|man|woman|apple|orange|\n",
    "|--|---|---|---|---|\n",
    "|gender|-1|1|0.001|0.001|\n",
    "|fruit|0.0001|0.0001|0.98|0.97|\n",
    "|...|...|...|...|...|\n",
    "|alive|1|1|0.99|0.98|\n",
    "\n",
    ", where $embedding\\ matrix\\ dim = n_{features} \\times n_{words}$\n",
    "\n",
    "Then instead of one-hot presentation, we can use these vectors to represent words:\n",
    "#### <center>Word Vectos</center>\n",
    "|e_man|e_woman|e_apple|e_orange|\n",
    "|---|---|---|---|\n",
    "|$\\begin{pmatrix}-1\\\\0.0001\\\\ ...\\\\ 1\\\\\\end{pmatrix}$|$\n",
    "\\begin{pmatrix}1\\\\0.0001\\\\ ...\\\\ 1\\\\\\end{pmatrix}$|$\n",
    "\\begin{pmatrix}0.001\\\\0.98\\\\ ...\\\\ 0.98\\\\\\end{pmatrix}$|$\n",
    "\\begin{pmatrix}0.001\\\\0.97\\\\ ...\\\\ 0.98\\\\\\end{pmatrix}$|\n",
    "\n",
    "Then the dimension of these vectors is the same as the number of features. There are high-dimensional (e.g. 50, 100, 200, 300) pretrained vectors for words available from resources like [GloVe](https://nlp.stanford.edu/projects/glove/) and [fastText](https://fasttext.cc/), or you can train your own model from large text corpus (1~10B) ([how](https://www.tensorflow.org/tutorials/representation/word2vec)). \n",
    "\n",
    "\n",
    "Note: $o_{word}E = e_{word} $, where $o_{word}$ is the one-hot encoding, $E$ is the embedding matrix, $e_{word}$ is the word vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Why word embeddings? \n",
    "The whole idea of word embedding is to embbed words in high dimensional space, so that similar words are grouped closer. For example, man, woman, queen, king would be grouped together, whereas cat, dog, frog would be grouped together somewhere else. Using t-SNE to map it on 2_dimensional space, it's easy to visualise the embedding: \n",
    "\n",
    "#### <center>Dense Representation</center>\n",
    "<img src='./img/tsne.png'>\n",
    "\n",
    "So the model will be able to generalise words better. For example, if we group uncommon word like durian together with other fruit words, then the model will know that durain is also a kind of fruit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another finding of word embedding is that it can learn word analogy relationshops of words pretty well. For example, it can learn that \"man is to woman\" is siilar to \"boy is to girl\". \n",
    "\n",
    "\n",
    "#### <center>Word Analogy</center>\n",
    "\n",
    "<img src='./img/analogy.jpg'>\n",
    "\n",
    "The linear relationship between pair is depicted by the difference between vactors of words. So the lines are almost parallel because the differences of vectors are almost the same, i.e. $e_{man} - e_{woman} \\approx e_{king} - e_{queen}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Model\n",
    "Suppose we have some context and want to predict the unknown word. For example:\n",
    "<center>I want a glass of orange ______ along with my cereal.</center>\n",
    "To predict the unknown word (i.e. juice), we can use everything before juice, i.e. \n",
    "<center>input = I want a glass of orange<center>   \n",
    "<center>  target = juice<center> \n",
    "    \n",
    "Or we can use a fixed history window, and slide the window i.e. \n",
    "<center>input = a glass of orange<center>\n",
    "<center>  target = juice<center> \n",
    "    \n",
    "Or we can use context from both left and right, i.e. \n",
    "<center>left input = a glass of orange, <center>  \n",
    "<center>right input = along with my cereal<center>  \n",
    "<center>  target = juice<center>    \n",
    "    \n",
    "For word embedding model, it's more effective using just one word, i.e.\n",
    "<center>input = orange <center>  \n",
    "<center>  target = juice<center>\n",
    "\n",
    "or \n",
    "\n",
    "<center>left input = orange <center>  \n",
    "<center>right input = along <center>  \n",
    "<center>  target = juice<center>\n",
    "\n",
    "\n",
    "So mathematically, word vectors are extracted from embedding matrix, then fed into some layers and finally passed to a softmax activation to predict which word is the most possible from dictionary. \n",
    "\n",
    "\n",
    "$$f(e_{a}, e_{glass}, e_{of}, e_{orange},W_1,b_1,a_0) = \\hat{y}_1$$, where $f$ is softmax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things more formal, depending on how we define input, we have the following different models:\n",
    "\n",
    "#### <center>Skip-grams<center>\n",
    "\n",
    "Given some context to predict some random word within some window (e.g. $\\pm 5$ words).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlsa",
   "language": "python",
   "name": "mlsa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
