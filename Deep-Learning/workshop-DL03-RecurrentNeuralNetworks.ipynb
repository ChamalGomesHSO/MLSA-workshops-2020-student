{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop DL03: Recurrent Neural Network\n",
    "\n",
    "## Agenda:\n",
    "- Introduction to RNN\n",
    "- Use RNN for text generation\n",
    "\n",
    "## Motivation: \n",
    "From time to time, beside from tabular data or image data we also need to deal with sequential data, for example, sentences, coordinates of a moving object, sound waves... \n",
    "\n",
    "Unlike pixel values or data from a table, sequnce-type data can be one of the hardest to deal with, because we need to: \n",
    "- handle sequences with varying length,\n",
    "- track long-term dependencies (e.g. long sentences), \n",
    "- preserve order (e.g. \"this is good, not bad at all\" versus \"this is bad, not good at all\"), and \n",
    "- share parameters across sequnence (so no need to relearn something if they appeared before in the sequence). \n",
    "\n",
    "As we talk about RNN we will see how RNN meets these requirements. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of RNN:\n",
    "\n",
    "|Type|Input|Output|Examples|\n",
    "|:--:|:---:|:----:|:-----:|\n",
    "|**One-to-many**| zero/one input| a sequence| music generation |\n",
    "|**Many-to-one**| a sequence | one output | sentiment classification  |\n",
    "|**Many-to-many**| a sequence | a sequence | speech recognition, machine translation|\n",
    "\n",
    "\n",
    "We will revisit these concepts after we learn more about RNN structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a RNN?\n",
    "A one-sentence description for it could be a loop of neural network layers. We will see more clearly when we discuss each property of RNN.\n",
    "#### Order Preservation\n",
    "When dealing with sequence data, we need to preserve the order of the components (e.g. words) in the sequence. Instead of putting everything in the bag and hand it to the model, we need the model to process the data in the right order. So we need to introduce some order or time component to the model, like this:\n",
    "\n",
    "<img src='./img/ordering.png'>\n",
    "\n",
    "\n",
    "Let's use an example of name identification, where the model needs to identify whether the words are names or not. When the word is part of the name, the model predicts 1, otherwise 0. Let's say $x_0$ is our first word, $x_1$ is second word ... and so on. $\\hat{y_0}$ is the prediction of the first word, $\\hat{y_1}$ is the prediction of the second word ... and so on. What's inside the green box is the hidden layers that the model revisits everytime it processes a new input, which is why it's also called **recurrent cell**. So the model can scan through the sentence from left to right by processing the words one at a time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies \n",
    "RNN not only preserves the order but also track dependencies. For each word in the $k^{th}$ position, its activation is not only passed to the $k^{th}$ prediction, but also passed to the ${k+1}^{th}, {k+2}^{th}, ...$ predictions. So the prediction at time $k$ uses the current information as well as the information from earlier sequence. (There is other model that uses information from later sequence, namely Bidirectional RNN.) For example, in the following graph, the second prediction uses all information up to the current one.\n",
    "\n",
    "<img src = './img/flow.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters sharing\n",
    "RNN also uses the same layers across the sequence, i.e. it shares the same set of parameters/weights thought-out.\n",
    "\n",
    "<img src = './img/sharing.png'>\n",
    "\n",
    "As we can see from the graph above, for each timestamp or order, RNN uses the exact same set of parameters and layer structure for prediction as well as activation to pass on to the next cell, i.e.\n",
    "$$a_t = g(W_{aa}a_{t-1}+W_{ax}x_t+b_a)$$\n",
    "$$\\hat{y_t} = f(W_{ay}a_t+b_y)$$\n",
    "for $t=1,2,...$ where $g$ is your choice of activation function (e.g. tanh), $f$ is the activation functions for the output (e.g. sigmoid, softmax), and $a_0$ is initialised activations (often initialised with zeros).\n",
    "\n",
    "**Vectorization version**: \n",
    "\n",
    "$$a_t = g(W_{a}[a_{t-1},x_t]+b_a)$$\n",
    "$$\\hat{y_t} = f(W_{ay}a_t+b_y)$$\n",
    "$where\\ W_a $ is horizontal stack of $W_{aa}\\ and\\ W_{ax}$, $[a_{t-1},x_t]$ is vertical stack of $a_{t-1}\\ and\\ x_t$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other representation of RNN\n",
    "This representation of RNN is also known as the **unrolled** version. Since the green box (recurrent cell) is revisited again and again for the whole sequence, sometime we can see simplified version like the one on the left:\n",
    "\n",
    "<img src = './img/loop.png'>\n",
    "\n",
    "If we unroll the internal loop, it's exactly the one on the right.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we train a RNN?\n",
    "Like other neural network, we train RNN through backpropagation, where we update parameters to minimise loss function by gradient descent. By the use of chain rule, this process is going backwards as the following graph shown. Since RNN has time element, its bakpropagation is also known as **bachpropagation through time** (cool name eh) as the gradient flows from right to left. \n",
    "\n",
    "<img src = './img/backprop-through-time.png'>\n",
    "\n",
    "Images sources: ©MIT 6.S191: Introduction to Deep Learning [introtodeeplearning.com](http://introtodeeplearning.com/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of RNN architectures\n",
    "Now that we have covered the basis of RNN, let's dig a little bit more about different types of RNN as mentioned before.\n",
    "\n",
    "|one-to-many|many-to-one|many-to-many|\n",
    "|:---------:|:---------:|:----------:|\n",
    "|<img src = './img/one-to-many.png'>|<img src = './img/many-to-one.png'>|<img src = './img/many-to-many.png'>|\n",
    "|music generation (given one note or nothing to generate a melody), text generation | sentiment classification (given comments to classify sentiment), DNA sequence analysis | speech recognition (given sound waves to recognise speech), machine translation |\n",
    "\n",
    "For one-to-many, usually the output in earlier timestamp is also passed to later timestamp. There is also a type called one-to-one, but nothing too exciting there except that it's a standard or vanilla neural network. \n",
    "\n",
    "Perhaps the most interesting one is many-to-many, where the input and output length can vary (e.g. machine translation), so it requires something called \"**encoder**\" and \"**decoder**\". <img src='./img/encoder-decoder.png'>There are other RNN structures like attention based architecture that are not captured in these diagrams, which we encourage you to explore it yourself.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep RNN\n",
    "Deep RNN is a stack of RNNs. This is an example of a 3-layer RNN:\n",
    "\n",
    "<img src='./img/deep-rnn.png'>\n",
    "\n",
    "Source: [deeplearning.ai](https://www.coursera.org/learn/nlp-sequence-models/lecture/ehs0S/deep-rnns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a RNN\n",
    "Now let's build a RNN model to generate some headlines!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.disable_eager_execution()\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Masking\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku \n",
    "\n",
    "# set seeds for reproducability\n",
    "from tensorflow import set_random_seed\n",
    "from numpy.random import seed\n",
    "set_random_seed(2)\n",
    "seed(1)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, time, string \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N.F.L. vs. Politics Has Been Battle All Season Long',\n",
       " 'Voice. Vice. Veracity.',\n",
       " 'A Stand-Up’s Downward Slide',\n",
       " 'New York Today: A Groundhog Has Her Day',\n",
       " 'A Swimmer’s Communion With the Ocean',\n",
       " 'Trail Activity',\n",
       " 'Super Bowl',\n",
       " 'Trump’s Mexican Shakedown',\n",
       " 'Pence’s Presidential Pet',\n",
       " 'Fruit of a Poison Tree']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the files\n",
    "curr_dir = './data/'\n",
    "corpus = []\n",
    "for filename in os.listdir(curr_dir):\n",
    "    article_df = pd.read_csv(curr_dir + filename)\n",
    "    corpus.extend(list(article_df.headline.values))\n",
    "    break\n",
    "\n",
    "corpus = [h for h in corpus if h != \"Unknown\"]\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 1: Data prep\n",
    "To train an RNN we need a large corpus(body or set) of text. Recall that in the classification workshop we have prepared the string-type data by encoding different string values into distint numbers. It works OK if we have small amount of string values, but what if we have a dictionary of words (10,000 to 1M words depending on the dictionary)? We're gonna do something known as **tokenization**. What it really means is that it forms a vocabulary/dictionary of the text and maps the sentence to the individual tokens. \n",
    "\n",
    "To tell the model when the sentence ends, we can add an extra token called **EOS** (end of sentence) at the end of the sentence. If some word is not known in our vocabulary, we can replace the word with a **UNK**(unknown) token instead of identifying it as a specific word. \n",
    "\n",
    "\n",
    "There is also a trend of tokenizing characters instead of words. [Here](https://www.tensorflow.org/tutorials/sequences/text_generation) is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[224, 159, 160, 123, 97, 78, 677, 678, 40, 27, 225],\n",
       " [226, 679, 680],\n",
       " [2, 227, 681, 682, 359],\n",
       " [11, 26, 28, 2, 683, 78, 161, 98],\n",
       " [2, 684, 685, 12, 1, 686],\n",
       " [360, 687],\n",
       " [228, 361],\n",
       " [20, 688, 689],\n",
       " [362, 363, 690],\n",
       " [691, 4, 2, 364, 692]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=None, \n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
    "                      lower=True, split=' ', char_level=False, \n",
    "                      oov_token=None, document_count=0) # these are the defaults\n",
    "# Keras has inbuilt model for tokenization \n",
    "# which can be used to obtain the tokens and their index in the corpus.\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "# Updates internal vocabulary based on a list of texts\n",
    "sequence = tokenizer.texts_to_sequences(corpus)\n",
    "#Transforms each text in texts to a sequence of integers\n",
    "print(len(sequence))\n",
    "sequence[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to construct RNN for texr generation. Here we show one way: give the network a sequence of words to guess what the next word is. So in terms of supervise models, we will use the fitst $n^{th}$ words as features and the ${n+1}^{th}$ word as label. Then use the $2^{th}$ ~ ${n+1}^{th}$ as features and ${n+2}^{th}$ as label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_from_sequence(sequence):\n",
    "    # determine how many words we use to guess the next word\n",
    "    training_length = 5 \n",
    "    \n",
    "    # initialising arrays with zeros\n",
    "    features = np.array([np.zeros([training_length])],dtype=np.int8)\n",
    "    labels = np.zeros([0],dtype=np.int8)\n",
    "    \n",
    "    \n",
    "    # Iterate through all the sequences of tokens\n",
    "    for seq in sequence:\n",
    " \n",
    "        # Create multiple training examples from each sequence\n",
    "        for i in range(training_length, len(seq)):\n",
    "            # extract components in the sequence\n",
    "            extract = seq[(i - training_length):(i + 1)]  \n",
    "            \n",
    "            # Set the features and label\n",
    "            feature = np.array(extract[:-1]) # take all but the last one\n",
    "            label = np.array(extract[-1]) # take the last one\n",
    "            \n",
    "            # stacking features and labels\n",
    "            features = np.append(features,[feature],axis=0) \n",
    "            labels = np.append(labels,[label],axis=0)\n",
    "    \n",
    "    # getting rid of the init zeros\n",
    "    features = np.array(features[1:])\n",
    "    labels = np.array(labels[1:])\n",
    "    return features, labels\n",
    "\n",
    "features, labels = get_features_from_sequence(sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1726, 5)\n",
      "(1725,)\n",
      "[[224 159 160 123  97]\n",
      " [159 160 123  97  78]\n",
      " [160 123  97  78 677]\n",
      " [123  97  78 677 678]\n",
      " [ 97  78 677 678  40]\n",
      " [ 78 677 678  40  27]\n",
      " [ 11  26  28   2 683]\n",
      " [ 26  28   2 683  78]\n",
      " [ 28   2 683  78 161]\n",
      " [  2 684 685  12   1]]\n",
      "[677 678  40  27 225  78 161  98 686   8]\n"
     ]
    }
   ],
   "source": [
    "# print first 10 features and labels\n",
    "print(np.shape(features));print(np.shape(labels))\n",
    "print(features[0:10]);print(labels[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1726, 2351)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of words in vocabulary\n",
    "num_words = len(tokenizer.index_word)+1\n",
    "\n",
    "# initialise array with zeros\n",
    "label_array = np.zeros((len(features),num_words),dtype=np.int8)\n",
    "\n",
    "#one hot encoding the labels\n",
    "for example_index, word_index in enumerate(labels):\n",
    "    label_array[example_index,word_index] = 1\n",
    "    \n",
    "print(label_array.shape)\n",
    "label_array[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we put data into the model, we also want to put in an extra layer call **embedding layer** that adds in extra knowledge about each word. Learn more in this [optional workshop](https://github.com/Phoebe0222/MLSA-workshops-2019-student/blob/master/Deep-Learning/workshop-DL04-optional-NLP-and-WordEmbedding.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in embeddings from https://nlp.stanford.edu/projects/glove/\n",
    "glove_vectors = './glove.6B.50d.txt'\n",
    "glove = np.loadtxt(glove_vectors, dtype='str', comments=None)\n",
    "\n",
    "# Extract the vectors and words\n",
    "vectors = glove[:, 1:].astype('float')\n",
    "words = glove[:, 0]\n",
    "\n",
    "# Create lookup of words to vectors\n",
    "word_lookup = {word: vector for word, vector in zip(words, vectors)}\n",
    "\n",
    "# New matrix to hold word embeddings\n",
    "embedding_matrix = np.zeros((num_words, vectors.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.11875,  0.6722 ,  0.19444,  0.55269,  0.53698, -0.37237,\n",
       "       -0.73494, -0.30575, -0.92601, -0.43276])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, word in enumerate(tokenizer.index_word.keys()):\n",
    "    # Look up the word embedding\n",
    "    vector = word_lookup.get(word, None)\n",
    "\n",
    "    # Record in matrix\n",
    "    if vector is not None:\n",
    "        embedding_matrix[i + 1, :] = vector\n",
    "# pretrained embedding for the word president        \n",
    "word_lookup['president'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 2: Design RNN\n",
    "So far we still haven't looked at what's really in the green box or recurrent cell. In a stadard RNN, what's inside is just one layer, but in more complicated models, there can be multiple layers interacting in a strange way. \n",
    "\n",
    "The original intention to build such layers is to address the problem known as **vanishing gradient**. It means in a long sentence or sequence, when we backpropagate through time we're multiplying a long chain of gradients and it's very likely that the product become so small that the gradients far down the sequence might have very little effect.\n",
    "\n",
    "A more intuitive way to interpret it is to look at the sentence. For example, in the sentence \"The cat, that already ate ..., was full\", \"the cat\" is singular and the \"...\" can be arbitrary long, so the RNN might not be able to detect the singular subject and correctly use singular \"was\". \n",
    "\n",
    "What if we can tell the model to hold some important information aside? This is exactly what **Gated Recurrent Unit** (GRU) and **Long Short Term Memory** (LSTM) do.\n",
    "\n",
    "These models introduce a **memory cell** state the stores important information like subject and pass it through a **gate** to later sequence where it really matters. And in the bakpropagation, we can use the cell state chain to calculate the gradients to avoid long product of gradients. \n",
    "\n",
    "Notations:\n",
    "\n",
    "- a **memory cell** state $c_0, c_1, ...c_t$ is a sequence of vector that contains value between 0 to 1 indicating whether the important word is stored in memory\n",
    "    - if $c_t = 1 $, it means \"keep the word, it's still important\"\n",
    "    - if $c_t = 0 $, it means \"forget abbout the word, it's useless now\"\n",
    "    - for example, in the sentence \"The cat, that already ate ..., was full\", if the \"cat\" was stored in the memory, when the model reaches the choice between \"was\" or \"were\", it remembers that the subject is singular, so it will chooses \"was\". I.e. $c_0(cat),c_1(cat),c_2(cat),...c_{t-1}(cat),c_t(cat)=0,1,1,...,1,0$\n",
    "\n",
    "- a **gate** decides whether to let the information through to protect and control the memory cell state; there are:\n",
    "    - update gate $u$: decides whether to update the cell state value with the canditate value\n",
    "        - if $u=1$, it means \"update the cell\"\n",
    "        - if $u=0$, it means \"do nothing\"\n",
    "    - relevant gate $r$: decides how relevant the current activation and new input is regarding to cell state\n",
    "    - forget gate $f$: decides whether to keep the old cell state value \n",
    "    - output gate $o$: controls the output of activation\n",
    "\n",
    "Let\n",
    "- $c_{t-1}$ be the current cell value, with the same dimemsion as activation\n",
    "- $\\tilde{c}_t$ be the canditate cell value for $c_t$, \n",
    "\n",
    "\n",
    "Then\n",
    "- **LSTM**: \n",
    "    - current cell: $c_{t-1}$\n",
    "    - canditate cell: $\\tilde{c}_t = tanh(W_c[a_{t-1},x_t]+b_c)$\n",
    "    - update gate: $u = \\sigma(W_u[a_{t-1},x_t]+b_u)$\n",
    "    - forget gate: $f=\\sigma(W_f[a_{t-1},x_t]+b_f)$\n",
    "    - new cell: $c_t = u*\\tilde{c}_t + f*c_{t-1}$\n",
    "    - output gate: $o = \\sigma(W_o[a_{t-1},x_t]+b_o) $\n",
    "    - new activation: $a_t=o*tanh(c_t)$ \n",
    "\n",
    "- **GRU**: \n",
    "    - current cell: $c_{t-1}  = a_{t-1}$\n",
    "    - canditate cell: $\\tilde{c}_t = tanh(W_c[r*a_{t-1},x_t]+b_c)$\n",
    "    - update gate: $u = \\sigma(W_u[a_{t-1},x_t]+b_u)$\n",
    "    - relevant gate: $r = \\sigma(W_r[a_{t-1},x_t]+b_r)$\n",
    "    - new cell: $c_t = u*\\tilde{c}_t + (1-u)*c_{t-1}$\n",
    "    - new activation: $a_t = c_t$\n",
    "\n",
    "\n",
    "A snapshot:\n",
    "<img src='./img/snapshot.png'>\n",
    "\n",
    "[This](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) is a really good blog on LSTM, which we highly recommand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 5, 50)             117550    \n",
      "_________________________________________________________________\n",
      "masking_3 (Masking)          (None, 5, 50)             0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2351)              152815    \n",
      "=================================================================\n",
      "Total params: 303,965\n",
      "Trainable params: 186,415\n",
      "Non-trainable params: 117,550\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "training_length = 5\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model.add(\n",
    "    Embedding(input_dim=num_words,\n",
    "              input_length = training_length,\n",
    "              output_dim=50,\n",
    "              weights=[embedding_matrix],\n",
    "              trainable=False,\n",
    "              mask_zero=True))\n",
    "\n",
    "# Masking layer for pre-trained embeddings\n",
    "model.add(Masking(mask_value=0.0))\n",
    "\n",
    "# Recurrent layer\n",
    "model.add(LSTM(64, return_sequences=False, \n",
    "               dropout=0.1, recurrent_dropout=0.1))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(64,activation='relu'))\n",
    "\n",
    "# Dropout for regularization\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(num_words, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 3: Model training\n",
    "Like any other model, we need to define the loss function based on prediction and actual output. In the example of language model, suppose we have a sentence consist of words $$y_0, y_1, ... y_t.$$ Then the prediction of each word is the set of the probabilities of the word being classified as all possibble words in the dictionary (e.g. a, aaron, ..., cat, ...UNK, EOS):\n",
    "$$\\hat{y}_t = P(token_i)\\ for\\ i=1,2,..,$$ \n",
    "Then loss function at each timestamp is defined by softmax function:\n",
    "$$L_t(\\hat{y}_t,y_t)=-\\sum_i y_t\\log{\\hat{y}_t}$$\n",
    "Since we have prepared $y_t$ by one-hot encoding, this equation can be reduced to $$L_t=-\\log{P(y_t)}$$\n",
    "Total loss function is:\n",
    "$$L=\\sum_t L_t$$\n",
    "Replacing $L_t$: \n",
    "$$=\\sum_t -\\log{P(y_t)}$$\n",
    "$$=-\\log{\\prod_t}P(y_t)$$\n",
    "\n",
    "Often we initialise with zeros, so $x_0 = \\vec{0}$, $x_1 = y_0$, $x_2 = y_1$... $x_t=EOS$\n",
    "\n",
    "<img src='./img/loss-function.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $\\prod_tP(y_t)$ in the loss function is also the probabbility of sentence. In a language-based RNN, **Language Modelling** is the fundamental for speech recognition and machine translation, which tells the probabilities of sentences. What it means is that it tells by how much the sentence is \"real\" comparing to sentences we pick up randomly from, say newspapera, emails, webpages etc. \n",
    "For example: $$P(\"The\\ apple\\ and\\ pear\\ salad\\ is\\ tasty.\") = 5.7\\times 10^{-10}$$ $$and$$ $$P(\"The\\ apple\\ and\\ pair\\ salad\\ is\\ tasty.\") = 3.2\\times 10^{-13}$$\n",
    "\n",
    "How we calculate the probability of sentence is by multiplying the conditional probabilities of words in the sentence. Let's say $y_1,y_2...y_t$ are the words from a sentence, then the probabbility of sentence is:\n",
    "$$P(y_1,y_2...y_t)=P(y_1)P(y_2)...P(EOS)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 5s - loss: 7.7495 - acc: 0.0440\n",
      "Epoch 2/10\n",
      " - 1s - loss: 7.7072 - acc: 0.0440\n",
      "Epoch 3/10\n",
      " - 1s - loss: 7.6686 - acc: 0.0440\n",
      "Epoch 4/10\n",
      " - 1s - loss: 7.6311 - acc: 0.0440\n",
      "Epoch 5/10\n",
      " - 1s - loss: 7.5948 - acc: 0.0440\n",
      "Epoch 6/10\n",
      " - 1s - loss: 7.5596 - acc: 0.0440\n",
      "Epoch 7/10\n",
      " - 1s - loss: 7.5255 - acc: 0.0440\n",
      "Epoch 8/10\n",
      " - 1s - loss: 7.4924 - acc: 0.0440\n",
      "Epoch 9/10\n",
      " - 1s - loss: 7.4605 - acc: 0.0440\n",
      "Epoch 10/10\n",
      " - 1s - loss: 7.4295 - acc: 0.0440\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(features, label_array, epochs=10, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'President Donald Trump was in the'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# given the seed_text and try to predict the next word\n",
    "seed_text = 'President Donald Trump was in'\n",
    "token_list = tokenizer.texts_to_sequences([seed_text])\n",
    "predicted = model.predict(np.array(token_list))\n",
    "output_word = \"\"\n",
    "for word,index in tokenizer.word_index.items():\n",
    "    if np.any(index == np.argmax(predicted)):\n",
    "        output_word = word\n",
    "        break\n",
    "seed_text += \" \"+output_word\n",
    "seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlsa",
   "language": "python",
   "name": "mlsa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
